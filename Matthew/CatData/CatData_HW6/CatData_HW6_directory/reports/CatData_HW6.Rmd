---
title: "CatData HW6"
author: "Matthew Vanaman"
date: '`r format(Sys.Date(), "%m-%d-%Y")`'
monofont: Courier New
output:
  pdf_document: 
    latex_engine: xelatex
  word_document: default
header-includes: \usepackage{tikz,xcolor,listings}
---
`r knitr::opts_knit$set(root.dir='..')`

```{r, include=FALSE, cache=TRUE}
library(ProjectTemplate); load.project()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
options(scipen = 999)
knitr::opts_chunk$set(engine.path = "/anaconda3/bin/python")
np <- reticulate::import("numpy")
sympy <- reticulate::import_from_path("sympy", path = "/anaconda3/lib/python3.7/site-packages")
```



A4.31 (5 points)

A4.16-A4.17, A5.4 (10 points)




# 4.3

**\large (a):** In the linear probability model, the coefficient is treated the same way as it is in linear regression. That is, it represents a slope such that for every unit increase x (decade, in this case), there is a corresponding unit change in y (the probability of pitching a complete game, in this case, the change of which = -0.0694).

**\large (b):** 
$$\hat \pi = 0.7578 - 0.0694(12),$$
$$= `r 0.7578 - 0.0694*(12)`.$$
Percentages can't be negative! So not possible.

**\large (c):** 
$$\hat \pi = \dfrac {e^{\alpha + \beta x}} {1 + e^{\alpha + \beta x}}.$$
$$\hat \pi = \dfrac {e^{1.148 - 0.315 (12)}} {1 + e^{1.148 - 0.315 (12)}},$$
$$= `r exp(1)^(1.148 - 0.315 * 12) / (1 + exp(1)^(1.148 - 0.315 * 12))`.$$
This is much more plausible. 


# 4.5

```{r, echo=FALSE}
shuttle <- as.data.frame(matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 
                    13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,
                    
                    66, 70, 69, 68, 67, 72, 73, 70, 57, 63, 
                    70, 78, 67, 53, 67, 75, 70, 81, 76, 79, 75, 76, 58,
                    
                    0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
                    1 ,0 ,0 ,0, 0, 0, 0, 1, 0, 1),
                  
                  ncol = 3, nrow=23))
colnames(shuttle) <- c("Ft", "Temp", "TD")
shuttle_mod <- glm(TD ~ Temp, family = binomial, data = shuttle)
summary(shuttle_mod)
```


**\large (a):** 

The model comes out to be `r round(coef(shuttle_mod)["(Intercept)"], 2)` + (`r round(coef(shuttle_mod)["Temp"], 2)`)$x$. When temperature is 0, the log odds are 15.04. The log odds decrease by 0.23 with every unit increase in temperature.  

**\large (b):** 
$$\hat \pi = \dfrac {e^{15.04 - 0.23 (31)}} {1 + e^{15.04 - 0.23(31)}}.$$
$$= `r exp(1)^(15.04 - 0.23 * 31) / (1 + exp(1)^(15.04 - 0.23 * 31))`.$$


**\large (c):**

You need to solve for x:
$$0.50 = \dfrac {e^{15.04 - 0.23 (x)}} {1 + e^{15.04 - 0.23(x)}}.$$
The temperature comes out to be 64.8. To get the linear approximation (i.e. rate of change), take the derivative of the logistic function above (with respect to x) to get its probability density function: 
$$f'(x) = \dfrac {\beta e^{\alpha + \beta x}} {(1 + e^{\alpha - \beta(x)})^2}$$ 
$$= \dfrac {15.04(e^{15.04 - 0.23 (64.8)})} {(1 + e^{15.04 - 0.23(64.8)})^2}$$ 
$$= `r (-0.2321627 * exp(1)^(15.0429016 - 0.2321627 * 64.79465)) / (1 + exp(1)^(15.0429016 - 0.2321627 * 64.79465))^2`.$$
At 64.8 degrees, for every unit increase in temperature, there is a 0.058 decrease in probability.

**\large (d):**

Since the coefficient is the log odds ratio, you exponentiate it to get the odds. Exp(-0.2321627) = `r exp(-0.2321627)`. This seems kind of difficult to interpret though, so it might be easier to take $1/OR = 1/0.79 = 1.27$ and say that for every unit increase in temperature, the odds of the o-rings *working correctly* increase by 1.27 - that is, every unit increase in temperature means there is 27% greater odds of the orings working the way they're supposed to (or 27% greater probability if we think events are rare enough to warrent using the odds ratio as an approximation of the risk ratio).

**\large (e):**

The Wald test is:
$$z^2 = \Bigg(\dfrac{\beta}{SE}\Bigg)^2,$$
$$= \Bigg(\dfrac{-0.2321627}{0.1082}\Bigg)^2,$$

$$= `r (-0.2321627/0.1082)^2`.$$
$z^2$ approximates the chi-square; the p-value is `r pchisq(4.6039476, df=1, lower.tail=FALSE)` with one degree of freedom.\newline

The likelihood ratio test is:
$$-2(\ell_0 - \ell_1),$$
$$-2(`r logLik(glm(TD ~ 1, family = binomial, data = shuttle))` `r logLik(shuttle_mod)`),$$
$$`r -2 * (logLik(glm(TD ~ 1, family = binomial, data = shuttle)) - logLik(shuttle_mod))`.$$
\newpage

$\chi^2$ p-value at 1 degree of freedom is `r pchisq(7.95196, df=1, lower.tail=FALSE)`. Temperature is probably important here.


## A4.15

### (a)

```{r, echo=FALSE}
district <- as.data.frame(c("NC", "NE", "NW", "SE", "SW", "NC", "NE", "NW", "SE", "SW"))
race <- as.data.frame(rep(c("Blacks", "Whites"), each = 5))
yes <- as.data.frame(c(24,10,5,16,7,47,45,57,54,59))
no <- as.data.frame(c(9,3,4,7,4,12,8,9,10,12))
extension <- as.data.frame(cbind(district, race, yes, no)); colnames(extension) <- c("district", "race", "yes", "no")
```

The CMH test is:
$$\dfrac {[\sum_k (n_{11k} - \mu_{11k})]^2} {\sum_k  \text{Var}(n_{11k})},$$
where:
$$\mu_{11k} = \dfrac {n_{1+k} n_{+1k}} {n_{++k}},$$
and:
$$\text{Var}_{11k} = \dfrac {n_{1+k} n_{2+k} n_{+1k} n_{+2k}}  {n^2_{++k} (n_{++k}-1)}.$$
```{r, echo=FALSE}
merit <- array(c(24,9,47,12,
                 10,3,45,8,
                 5,4,57,9,
                 16,7,54,10,
                 7,4,59,12),
               dim = c(2,2,5),
               dimnames = list(
                 meritPay = c("yes", "no"),
                 race = c("black", "white"),
                 district = c("NC", "NE", "NW", "SE", "SW")))

cmh <- mantelhaen.test(merit, z=district, correct = FALSE, exact = FALSE)
```

The CMH test statistic comes out to be `r cmh$statistic` with 1 df, and a p-value of `r cmh$p.value`. We reject the null hypothesis that merit pay decisions are independent of race.

### (b)
```{r, echo=FALSE}
#logistic regression model
merit_mod <- glm((yes/(yes + no)) ~ race , family = binomial, weights = yes + no, data = extension)
```
You could use the wald test to test the difference between the race parameter and the intercept while excluding distrit from the model. This would mean the intercept is the effect of race when race is Black, and the parameter is the effect when race is White. The parameter for White is zero according to the nuul. The wald test is $\Big(\frac {\beta} {SE}\Big)^2$, which comes out the be `r ((coef(merit_mod)[("raceWhites")] / (summary(merit_mod)$coefficients[2, 2]))^2)` with a p-value of `r pchisq(8.4655366, df=1, lower.tail=FALSE)`. We reject the null of no difference between race in merit decision pay.

### (c)

Unlike with the CMH, the model tells you the log-odds ratio which you can exponentiate to get the odds ratio. This tells us the magnitude of difference in decision pay rates between whites and blacks. Exponentiating the parameter for Whites, you get `r exp((coef(merit_mod)[("raceWhites")]))`, meaning Whites have 123% greater odds of getting a pay increase.