---
title: "CatData HW6"
author: "Matthew Vanaman"
date: '`r format(Sys.Date(), "%m-%d-%Y")`'
monofont: Courier New
output:
  pdf_document: 
    latex_engine: xelatex
  word_document: default
header-includes: \usepackage{listings, setspace, caption}
---
`r knitr::opts_knit$set(root.dir='..')`

```{r, include=FALSE, cache=TRUE}
library(ProjectTemplate); load.project()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
options(scipen = 999, xtable.comment = FALSE)
knitr::opts_chunk$set(engine.path = "/anaconda3/bin/python")
np <- reticulate::import("numpy")
sympy <- reticulate::import_from_path("sympy", path = "/anaconda3/lib/python3.7/site-packages")
```

\captionsetup[table]{labelformat=empty}


4.17, A5.4 (10 points)

# 4.3

**\large (a):** In the linear probability model, the coefficient is treated the same way as it is in linear regression. That is, it represents a slope such that for every unit increase x (decade, in this case), there is a corresponding unit change in y (the probability of pitching a complete game, in this case, the change of which = -0.0694).

**\large (b):** 
$$\hat \pi = 0.7578 - 0.0694(12),$$
$$= `r 0.7578 - 0.0694*(12)`.$$
Percentages can't be negative! So not possible.

**\large (c):** 
$$\hat \pi = \dfrac {e^{\alpha + \beta x}} {1 + e^{\alpha + \beta x}}.$$
$$\hat \pi = \dfrac {e^{1.148 - 0.315 (12)}} {1 + e^{1.148 - 0.315 (12)}},$$
$$= `r exp(1)^(1.148 - 0.315 * 12) / (1 + exp(1)^(1.148 - 0.315 * 12))`.$$
This is much more plausible. 


# 4.5

```{r, echo=FALSE}
shuttle <- as.data.frame(matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 
                    13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,
                    
                    66, 70, 69, 68, 67, 72, 73, 70, 57, 63, 
                    70, 78, 67, 53, 67, 75, 70, 81, 76, 79, 75, 76, 58,
                    
                    0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
                    1 ,0 ,0 ,0, 0, 0, 0, 1, 0, 1),
                  
                  ncol = 3, nrow=23))
colnames(shuttle) <- c("Ft", "Temp", "TD")
shuttle_mod <- glm(TD ~ Temp, family = binomial, data = shuttle)
```


**\large (a):** 

The model comes out to be `r round(coef(shuttle_mod)["(Intercept)"], 2)` + (`r round(coef(shuttle_mod)["Temp"], 2)`)$x$. When temperature is 0, the log odds are 15.04. The log odds decrease by 0.23 with every unit increase in temperature.  

**\large (b):** 
$$\hat \pi = \dfrac {e^{15.04 - 0.23 (31)}} {1 + e^{15.04 - 0.23(31)}}.$$
$$= `r exp(1)^(15.04 - 0.23 * 31) / (1 + exp(1)^(15.04 - 0.23 * 31))`.$$


**\large (c):**

You need to solve for x:
$$0.50 = \dfrac {e^{15.04 - 0.23 (x)}} {1 + e^{15.04 - 0.23(x)}}.$$
The temperature comes out to be 64.8. To get the linear approximation (i.e. rate of change), take the derivative of the logistic function above (with respect to x) to get its probability density function: 
$$f'(x) = \dfrac {\beta e^{\alpha + \beta x}} {(1 + e^{\alpha - \beta(x)})^2}$$ 
$$= \dfrac {15.04(e^{15.04 - 0.23 (64.8)})} {(1 + e^{15.04 - 0.23(64.8)})^2}$$ 
$$= `r (-0.2321627 * exp(1)^(15.0429016 - 0.2321627 * 64.79465)) / (1 + exp(1)^(15.0429016 - 0.2321627 * 64.79465))^2`.$$
At 64.8 degrees, for every unit increase in temperature, there is a 0.058 decrease in probability.

**\large (d):**

Since the coefficient is the log odds ratio, you exponentiate it to get the odds. Exp(-0.2321627) = `r exp(-0.2321627)`. This seems kind of difficult to interpret though, so it might be easier to take $1/OR = 1/0.79 = 1.27$ and say that for every unit increase in temperature, the odds of the o-rings *working correctly* increase by 1.27 - that is, every unit increase in temperature means there is 27% greater odds of the orings working the way they're supposed to (or 27% greater probability if we think events are rare enough to warrent using the odds ratio as an approximation of the risk ratio).

**\large (e):**

The Wald test is:
$$z^2 = \Bigg(\dfrac{\beta}{SE}\Bigg)^2,$$
$$= \Bigg(\dfrac{-0.2321627}{0.1082}\Bigg)^2,$$

$$= `r (-0.2321627/0.1082)^2`.$$
$z^2$ approximates the chi-square; the p-value is `r pchisq(4.6039476, df=1, lower.tail=FALSE)` with one degree of freedom.\newline

The likelihood ratio test is:
$$-2(\ell_0 - \ell_1),$$
$$-2(`r logLik(glm(TD ~ 1, family = binomial, data = shuttle))` `r logLik(shuttle_mod)`),$$
$$`r -2 * (logLik(glm(TD ~ 1, family = binomial, data = shuttle)) - logLik(shuttle_mod))`.$$

$\chi^2$ p-value at 1 degree of freedom is `r pchisq(7.95196, df=1, lower.tail=FALSE)`. Temperature is probably important here.


## A4.15

### (a)

```{r, echo=FALSE}
district <- as.data.frame(c("NC", "NE", "NW", "SE", "SW", "NC", "NE", "NW", "SE", "SW"))
race <- as.data.frame(rep(c("Blacks", "Whites"), each = 5))
yes <- as.data.frame(c(24,10,5,16,7,47,45,57,54,59))
no <- as.data.frame(c(9,3,4,7,4,12,8,9,10,12))
extension <- as.data.frame(cbind(district, race, yes, no)); colnames(extension) <- c("district", "race", "yes", "no")
```

The CMH test is:
$$\dfrac {[\sum_k (n_{11k} - \mu_{11k})]^2} {\sum_k  \text{Var}(n_{11k})},$$
where:
$$\mu_{11k} = \dfrac {n_{1+k} n_{+1k}} {n_{++k}},$$
and:
$$\text{Var}_{11k} = \dfrac {n_{1+k} n_{2+k} n_{+1k} n_{+2k}}  {n^2_{++k} (n_{++k}-1)}.$$
```{r, echo=FALSE}
merit <- array(c(24,9,47,12,
                 10,3,45,8,
                 5,4,57,9,
                 16,7,54,10,
                 7,4,59,12),
               dim = c(2,2,5),
               dimnames = list(
                 meritPay = c("yes", "no"),
                 race = c("black", "white"),
                 district = c("NC", "NE", "NW", "SE", "SW")))

cmh <- mantelhaen.test(merit, z=district, correct = FALSE, exact = FALSE)
```

The CMH test statistic comes out to be `r cmh$statistic` with 1 df, and a p-value of `r cmh$p.value`. We reject the null hypothesis that merit pay decisions are independent of race.

### (b)
```{r, echo=FALSE}
#logistic regression model
merit_mod <- glm((yes/(yes + no)) ~ race , family = binomial, weights = yes + no, data = extension)
```
You could use the wald test to test the difference between the race parameter and the intercept while excluding distrit from the model. This would mean the intercept is the effect of race when race is Black, and the parameter is the effect when race is White. The parameter for White is zero according to the nuul. The wald test is $\Big(\frac {\beta} {SE}\Big)^2$, which comes out the be `r ((coef(merit_mod)[("raceWhites")] / (summary(merit_mod)$coefficients[2, 2]))^2)` with a p-value of `r pchisq(8.4655366, df=1, lower.tail=FALSE)`. We reject the null of no difference between race in merit decision pay.

### (c)

Unlike with the CMH, the model tells you the log-odds ratio which you can exponentiate to get the odds ratio. This tells us the magnitude of difference in decision pay rates between whites and blacks. Exponentiating the parameter for Whites, you get `r exp((coef(merit_mod)[("raceWhites")]))`, meaning Whites have 123% greater odds of getting a pay increase.
\newpage

## 4.31 

```{r, echo=FALSE}

maryj <- data.frame(c("Yes", "Yes", "No", "No"),
                                c("Yes", "No", "Yes", "No"),
                                c(as.integer(c(911, 44, 3, 2))), 
                                c(as.integer(c(538, 456, 43, 279))))
colnames(maryj) <- c("AlcUse", "CigUse", "MaryjYes", "MaryjNo")

mary_intercept <- glm(MaryjYes/(MaryjYes+MaryjNo) ~ 1, weights = MaryjYes + MaryjNo, family = binomial, data = maryj)
mary_cig <- glm(MaryjYes/(MaryjYes+MaryjNo) ~ CigUse, weights = MaryjYes + MaryjNo, family = binomial, data = maryj)
mary_cig_alc <- glm(MaryjYes/(MaryjYes+MaryjNo) ~ CigUse + AlcUse, weights = MaryjYes + MaryjNo, family = binomial, data = maryj)
mary_full <- glm(MaryjYes/(MaryjYes+MaryjNo) ~ CigUse * AlcUse, weights = MaryjYes + MaryjNo, family = binomial, data = maryj)

# print good looking p-values
pvalr <- function(pvals, sig.limit = .001, digits = 3, html = FALSE) {

  roundr <- function(x, digits = 1) {
    res <- sprintf(paste0('%.', digits, 'f'), x)
    zzz <- paste0('0.', paste(rep('0', digits), collapse = ''))
    res[res == paste0('-', zzz)] <- zzz
    res
  }

  sapply(pvals, function(x, sig.limit) {
    if (x < sig.limit)
      if (html)
        return(sprintf('&lt; %s', format(sig.limit))) else
          return(sprintf('< %s', format(sig.limit)))
    if (x > .1)
      return(roundr(x, digits = 2)) else
        return(roundr(x, digits = digits))
  }, sig.limit = sig.limit)
}
```

\begin{doublespace}

\newcommand{\forceindent}{\leavevmode{\parindent=2em\indent}}

\forceindent The odds ratio of marijuana use for alcohol users vs non-users, ignoring whether or not they smoke, is `r round(((911+44)*(43+279)) / ((2+3) * (538 + 456)),2)`. The odds ratio of marijuana use for smokers  vs non-smokers, ignoring whether or not they drink, is `r round(((911+3)*(456+279)) / ((44+2) * (538 + 43)),2)`. These are pretty large, and may be deceptive because they are not conditioned on the other variable. On theoretical grounds, there may be reason to suspect that the use of cigarettes in addition to alcohol (or vice versa) has a compounding effect on the odds of marijuana use, so it makes sense to run a logistic regression with main effects for cigarette use and alcohol use as well as a cigarette x alchohol interaction. Using the likelihood ratio test $(-2(\ell_0 - \ell_1))$, we should first test to see whether we gain anything from adding cigarettes to an intercept-only model. We get a LRT statistic `r round(-2 * (logLik(mary_intercept) - logLik(mary_cig)),2)`, \textit{p} `r pvalr(pchisq((-2 * (logLik(mary_intercept) - logLik(mary_cig))), df=1, lower.tail=FALSE))` with 1 degree of freedom, strongly suggesting that the cigarette model is an improvement over the intercept-only model. Big surprise. But what about adding alcohol to the model? The LRT is `r round(-2 * (logLik(mary_cig) - logLik(mary_cig_alc)),2)` \textit{p} `r pvalr(pchisq((-2 * (logLik(mary_cig) - logLik(mary_cig_alc))), df=1, lower.tail=FALSE))` with 1 degree of freedom. It would thus be wise to include alcohol in the model. Lastly, what about adding an interaction term? Adding the interaction, the LRT is `r round(-2 * (logLik(mary_cig_alc) - logLik(mary_full)),2)` \textit{p} = `r pvalr(pchisq((-2 * (logLik(mary_cig_alc) - logLik(mary_full))), df=1, lower.tail=FALSE))` with 1 degree of freedom. The model does not seem to improve, and that the interaction term does not come out significant in this model serves as further evidence in favor of its exclusion. However, for substantive reasons, I think it is still informative to have in the model for reasons discussed below.\newline
\forceindent Examining the main-effects plus interaction model, with alcohol, cigarette, and alcohol+cigarette use predicting the use of marijuana, we find that the conditional odds ratio for marijuana use is `r round(exp((coef(mary_full)[("CigUseYes")])),2)`, \textit{p} = `r pvalr(coef(summary(mary_full))[2,4], digits = 3)` for smokers vs non-smokers, such that smokers have 9.73 greater odds of marijuana use compared to non-sokers. This is large, but not nearly as large as the unconditional odds ratio. For alcohol, the conditional odds ratio is `r round(exp((coef(mary_full)[("AlcUseYes")])),2)`, \textit{p} `r pvalr(coef(summary(mary_full))[3,4])` which is again quite sizable but not as large as the unconditional odds ratio. Importantly though, the odds ratio for the interaction is `r round(exp((coef(mary_full)[("CigUseYes:AlcUseYes")])),2)`, \textit{p} = `r pvalr(coef(summary(mary_full))[4,4], digits = 3)`. Because we fail to reject the null here, we continue to assume the null is true, thus that the odds ratio difference here is due to sampling variability. Despite the lack of statistical significance, this is still informative to have in the model because there may be practical reasons why you'd want to point out that adding smoking on top of drinking (or vice versa) doesn't really affect the odds of smoking marijuana. For example, an intervention that aims to prevent alcohol users from picking up smoking might not be worth the money, and that that money might be better spent on trying to prevent alcohol use in people who don't smoke and cigarette use in people who don't drink.

\end{doublespace}

## 4.16




### (a)

```{r, echo=FALSE, results="asis"}
# dataset
myers <- data.frame(c("t", "t", "f", "f", "t", "t", "f", "f", 
                      "t", "t", "f", "f", "t", "t", "f", "f"),
                    
                    c("j", "p", "j", "p", "j", "p", "j", "p",
                      "j", "p", "j", "p", "j", "p", "j", "p"),
                    
                    c("e", "e", "e", "e", "e", "e", "e", "e",
                      "i", "i", "i", "i", "i", "i", "i", "i"),
     
                    c("s", "s", "s", "s", "n", "n", "n", "n", 
                      "s", "s", "s", "s", "n", "n", "n", "n"),
                    
                    c(as.integer(c(10, 8, 5, 7, 3, 2, 4, 15, 
                                   17, 3, 6, 4, 1, 5, 1, 6))),
                    
                    c(as.integer(c(67, 34, 101, 72, 20, 16, 27, 
                                   65, 123, 49, 132, 102, 12, 30, 30, 73))))
colnames(myers) <- c("TF", "JP", "EI", "SN", "AlcYes", "AlcNo")
myers_table <- print(xtable::xtable(myers, caption="MBTI Dataset",align = "ccccccc"), caption.placement ="top")

# model
AlcUse <- myers$AlcYes/(myers$AlcYes + myers$AlcNo)
mbti_mod <- summary(glm(AlcUse ~ TF + JP + 
                          EI + SN, 
                        weights = AlcYes + AlcNo, family = binomial, data = myers))
mbti_mod_table <- print(xtable::xtable(mbti_mod, caption="MBTI Model Output", align="lcccc"), caption.placement ="top")

# coefficients
intercept <- coef(mbti_mod)[("(Intercept)"),1]
EI <- coef(mbti_mod)[("EIi"),1]
SN <- coef(mbti_mod)[("SNs"),1]
TF <- coef(mbti_mod)[("TFt"),1]
JP <- coef(mbti_mod)[("JPp"),1]
```




The model formula is $\text{logit}(\hat \pi) = `r round(intercept,2)` `r round(EI,2)`(E/I) `r round((SN),2)`(S/N) + `r round(TF,2)`(T/F) + `r round((JP),2)`(J/P)$. The intercept is the ENFJ type; that is, when EI = E, SN = N, TF = F, and JP = J.

### (b)
For an ESTJ, $$\hat \pi = \dfrac {e^{`r round(intercept,2)` - `r round(JP,2)` `r round(SN,2)`}}{(1 + e^{`r round(intercept,2)` - `r round(JP,2)` `r round(SN,2)`})},$$
$$= `r round(exp(1)^(intercept - JP - SN) / (1 + round(exp(1)^(intercept - JP - SN),2)),2)`.$$

### (c)

```{r, echo=FALSE}
x <- round(intercept + TF + JP,2)
```

The formula for ENTP would come out to be $\text{logit}(\hat \pi) = `r round((intercept),2)` + `r round(TF,2)` + `r round((JP),2)`$. To get the probability,
$$\hat \pi = \dfrac{e^{`r round(intercept,2)` + `r round(TF,2)` + `r round(JP,2)`}}{(1 + e^{`r round(intercept,2)` + `r round(TF,2)` + `r round(JP,2)`})},$$
$$= `r round(exp(x) / (1 + exp(x)),2)`.$$

## 4.17

### (a)

In table 4.14, the intercept captures Introverted Feelers. To get $\hat \pi$ for introverted feelers::
$$\hat \pi = \dfrac{e^{-2.8291}} {(1 + e^{-2.8291)}},$$
$$= `r round(exp(-2.8291) / (1 + exp(-2.8291)),2)`.$$

### (b)

The estimated conditional odds for the EI variable is
$$e^{`r round(0.5805,2)`} = `r round(exp(1)^(0.5805),2)`.$$

### (c)

To get CIs for the conditional odds ratio:
$$(e^{0.1589}, e^{1.0080}) = (`r round(exp(1)^(0.1589),2)`, \ `r round(exp(1)^(1.0080),2)`).$$


### (d)

We just need the inverse of part (b), so
$$\dfrac{1}{1.79} = `r round(1/1.79,2)`,$$ 
$$\text{CI} = (\dfrac{1}{2.74}, \dfrac{1}{1.17}),$$
$$ = (`r round(1/2.74,2)`, \ `r round(1/1.17,2)`).$$


### (e)

You could calculate the likelihood ratio test by taking $-2(\ell_0 - \ell_1)$, where $\ell$ is the model sans EI and $\ell_1$ is the model with EI. This approximates the chi-squared, and luckily the output has already done this procedure for us. The LR statistic is 7.28, which has a p-value of `r pvalr(pchisq(7.28, df=1, lower.tail=FALSE))`. We conclude that EI has  effect on the response above and beyond TF.
