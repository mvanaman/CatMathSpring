---
title: "CatData HW3"
author: "Matthew Vanaman"
date: "03/11/19"
monofont: Courier New
output:
  pdf_document: 
    latex_engine: xelatex
  word_document: default
header-includes: \usepackage{tikz,xcolor,listings}
---
`r knitr::opts_knit$set(root.dir='..')`

```{r, include=FALSE}
library(ProjectTemplate); load.project()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
options(scipen = 999)
```

A2.2, A2.5, A2.13, A2.21, A2.36. (3 points each)

A2.33 (5 points)

*All code and work are shown in the appendix.*

# 2.2

## (a)
**Answer:**\newline
Recoding this in a way that makes more sense to me. Will leave X and Y as they are:\newline
$X (0 = \textrm {no disease}, 1 = \textrm {disease}$\newline

$Y (0 = \textrm {negative}, 1 = \textrm {positive}$\newline

$\textrm {SENSITIVITY} = \pi_1 = P(Y = 1|X = 1) = \textrm {probability of positive diagnosis given you actually have the disease}$\newline

$\textrm {SPECIFICITY} = 1 - \pi_2 = 1 - P(Y = 1|X = 0) = \textrm {1 - the probability of positive diagnosis given you do not have disease}$\newline 

After subtracting away the  probability of positive diagnosis given no disease, you are left with probability of negative diagnosis given no disease:\newline
$1 - P(Y = 1|X = 0) =  P(Y = 0|X = 0) = \textrm {probability of negative diagnosis given you do not have the disease}$\newline

Likewise, the "noise" of the test is captured by 1 - specificity. A good test would want to show that 1 - specificity yields a large probability as an effective test will have a large probability of correctly diagnosing a patient that does not have the disease. \newline

## (b)
**Answer:**\newline
$P(X = 1|Y = 1) = \frac {P(Y = 1|X = 1)P(X = 1)} {P(Y = 1|X = 1)P(X = 1) + P(Y = 1|X = 2) P(X = 2)}$\newline



**Work:**\newline
$$\frac {\pi_1 \gamma} {[ \pi_1 \gamma + \pi_2 (1 - \gamma) ]},$$\newline

Using Bayes' rule:\newline

$$P(A|B = \frac {P(B|A)P(A)} {P(B)}$$\newline

In the above equation, P(B) represents P(X=1), or the probability of having the disease. But this is an  unknown value, expressed as $\gamma$. So we have to use a particular version of Bayes' rule that takes into account the conditional probabilities associated with P(B):
$$P(A|B) = \frac {P(B|A)P(A)} {P(B|A)P(A) + P(B| \widetilde A) P(\widetilde A)}$$\newline


Just replace with the problem's notation:
$$P(X = 1|Y = 1) = \frac {P(Y = 1|X = 1)P(X = 1)} {P(Y = 1|X = 1)P(X = 1) + P(Y = 1|X = 2) P(X = 2)}$$\newline

## (c)

