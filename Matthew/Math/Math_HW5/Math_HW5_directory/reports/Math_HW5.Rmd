---
title: "Math HW5"
author: "Matthew Vanaman"
date: '`r format(Sys.Date(), "%m-%d-%Y")`'
monofont: Courier New
output:
  pdf_document: 
    latex_engine: xelatex
  word_document: default
header-includes: \usepackage{tikz,xcolor,listings}
---
`r knitr::opts_knit$set(root.dir='..')`

```{r, include=FALSE}
library(ProjectTemplate); load.project()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
options(scipen = 999)
knitr::opts_chunk$set(engine.path = "/anaconda3/bin/python")
np <- reticulate::import("numpy")
sympy <- reticulate::import_from_path("sympy", path = "/anaconda3/lib/python3.7/site-packages")
```

4.9, 5.1, 5.2, 5.3

# 4.8
## (a)



### Work 

#### First Derivative:
The only layer here is in the exponent of $e$; everything else is constant.
$$f (x) = \dfrac {1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}, $$
$$A = -\frac {x^2}{2}, \ \ f(x) = \dfrac {1} {\sqrt{2 \pi}} e^A $$
$$A' = \dfrac {d}{dx} \Big( -\dfrac{x^2}{2} \Big) \\ = - \dfrac {1}{2} \Big(2x^{2-1}) \\ = - \dfrac {2x^{2-1}}{2} \\ = -x. $$
$$ \dfrac {1} {\sqrt{2 \pi}} e^A \Big(-x \Big) = \dfrac {-x} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}. $$

#### Second Derivative

Because the first derivative was multiplicative, we need the product to get the second derivative. \newline
Product rule: $f''(x) = g'(x) f(x) + g(x) f'(x).$ \newline
$f(x) =  \frac {1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}.$ \newline
$f'(x) = \frac {-x} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}.$ \newline
$g(x) = -x.$\newline
$g'(x) = \frac {d}{dx}(-x) = -1 .$

$$f''(x) =  - 1 \bigg( \dfrac {1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}} \bigg) - x \bigg( \dfrac {-x^2} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}\bigg)$$ 
$$= -\bigg( \dfrac {1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}} \bigg) +  \bigg( \dfrac {x^2} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}\bigg)$$
$$= \bigg( \dfrac {x^2 - 1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}\bigg).$$

#### Plug in 0 for each version

$$f(0) = \dfrac {1} {\sqrt{2 \pi}} e^{-\frac {0^2}{2}} = \dfrac {1}{\sqrt{2 \pi}}. $$
$$f'(0) = \dfrac {0^2} {\sqrt{2 \pi}} e^{-\frac {0^2}{2}} = 0. $$
$$f''(0) = \dfrac {0^2 - 1} {\sqrt{2 \pi}} e^{-\frac {0^2}{2}} = \dfrac {-1} {\sqrt{2 \pi}}. $$\newline
Taylor approximation - just plug in what we found:
$$f(x) \approx \dfrac {1}{\sqrt{2 \pi}} + 0x + \dfrac {-1 / \sqrt{2 \pi}} {2}x^2.$$
$$f''(x) \approx \dfrac {-1}{\sqrt{8 \pi}}x^2 + \dfrac {1} {\sqrt{2 \pi}}. $$

## (b)

```{r, echo=FALSE}
x <- seq(-3, 3, length=1000)
norm <- dnorm(x, mean=0, sd=1)
taylor <- (-1/sqrt(8*c(pi)))*x^2 + (1/sqrt(2*c(pi)))
df <- as.data.frame(cbind(x,norm,taylor))
ggplot(df, aes(x)) +
  geom_line(aes(y = norm, color = "Normal")) +
  geom_line(aes(y = taylor, color = "Taylor\nApproximation")) +
  ylab("") + xlab("") +
  labs(color = "Function") +
  theme_minimal()
```

Taylor series seems most accurate between about -1 and 1.


# 4.9
## (a)

Start with
$$\ln \bigg(\prod_{i = 1}^{N} p^{y_{i}} (1 - p)^{1 - y_{i}} \bigg), $$
$$\sum_{i = 1}^{N} \ln \bigg( p^{y_{i}} (1 - p)^{1-y_i} \bigg), $$
$$\sum_{i = 1}^{N} \bigg( \ln \big( p^{y_{i}} \big) + \ln\big( (1 - p)^{1-y_i} \big) \bigg), $$
$$\sum_{i = 1}^{N} \bigg( (y_i) \ln \big(p \big) + \ln\big( (1 - p)^{1-y_i} \big) \bigg), $$
$$\sum_{i = 1}^{N} \bigg( (y_i) \ln \big(p \big) + (1-y_i) \ln (1 - p) \bigg), $$
$$\sum_{i = 1}^{N} (y_i) \ln \big(p \big) + \sum_{i = 1}^{N}(1-y_i) \ln (1 - p), $$
$$\ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \sum_{i = 1}^{N}(1-y_i), $$
$$\ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \bigg(\sum_{i = 1}^{N}1\bigg)- \bigg(\sum_{i = 1}^{N}y_i\bigg), $$
$$\ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \bigg(N - \sum_{i = 1}^{N}y_i\bigg). $$


## (b)

$$\dfrac {d}{dp} \Bigg( \ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \bigg(N - \sum_{i = 1}^{N}y_i\bigg) \Bigg), $$
$$\dfrac {d}{dp} \Bigg( \ln \big(p \big) \sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {d}{dp} \Bigg( \ln (1 - p) \bigg(N - \sum_{i = 1}^{N}y_i\bigg) \Bigg), $$
$$\dfrac {d}{dp} \Bigg( \ln \big(p \big) \Bigg) \dfrac {d}{dp} \Bigg(\sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {d}{dp} \Bigg( \ln (1 - p)\Bigg) \dfrac {d}{dp} \Bigg(N - \sum_{i = 1}^{N}y_i \Bigg), $$
$$\dfrac {d}{dp} \Bigg( \ln \big(p \big) \Bigg) \dfrac {d}{dp} \Bigg(\sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {d}{dp} \Bigg( \ln (1 - p)\Bigg) \dfrac {d}{dp} \Bigg(N - \sum_{i = 1}^{N}y_i \Bigg), $$
$$\dfrac {1}{p} \Bigg(\sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {d}{dp} \Bigg( \ln (1 - p)\Bigg) \Bigg(N - \sum_{i = 1}^{N}y_i \Bigg). $$
Gotta use the chain rule:
$$ \dfrac {d}{dp} \Bigg( \ln (1 - p)\Bigg) = \bigg(\dfrac {1}{1 - p}\bigg)\dfrac {d}{dp}(1 - p) = \dfrac{ \frac {d}{dp}(1) - \frac {d}{dp}(p)} {1 - p} = \dfrac {0-1}{1-p} = -\dfrac {-1}{1-p}. $$
$$\dfrac {1}{p} \Bigg(\sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {-1}{1- p} \Bigg(N - \sum_{i = 1}^{N}y_i \Bigg), $$
$$\dfrac {1}{p} \Bigg(\dfrac {\sum_{i = 1}^{N} (y_i)}{1} \Bigg) + \dfrac {-1}{1- p} \Bigg(\dfrac {N - \sum_{i = 1}^{N}y_i}{1} \Bigg), $$
$$\dfrac {\sum_{i = 1}^{N} (y_i)}{p} - \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}. $$

## (c)

$$0 = \dfrac {\sum_{i = 1}^{N} (y_i)}{p} - \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}, $$
$$\dfrac {\sum_{i = 1}^{N} (y_i)}{p} = \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}, $$
$$\dfrac {\sum_{i = 1}^{N} (y_i)}{p} = \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}, $$
$$ \sum_{i = 1}^{N} y_i(1 -p) = p \bigg(N - \sum_{i = 1}^{N}y_i \bigg), $$
$$ \sum_{i = 1}^{N} y_i - \sum_{i = 1}^{N} y_i (p) = (p)N - (p)\sum_{i = 1}^{N}y_i, $$
$$ \sum_{i = 1}^{N} y_i = (p)N, $$
$$\dfrac {\sum_{i = 1}^{N} y_i}{N} = p. $$
In the maximum likelihood framework, the proportion observed in the data tracks the probability of in the population, assuming assumptions hold. In this case, we have a sample where we add up all of the y values, the divide by the number of values. This gives us the sample proportion (or mean), which is an estimate of the probability.

## (d)

If $ p = \frac {\sum_{i = 1}^{N} y_i}{N} $, and we're given a function that gives us p, then all we have to do is substitute the function for p, then substitute $x_i$ into that function.
$$p = \dfrac {1}{1 + e^{-(0.2 + 0.5x_i)}}. $$
Very Conservative:
$$p = \dfrac {1}{1 + e^{-(0.2 + 0.53)}} = 0.85. $$
```{r}
## calculations
# get euler's number
exp(1)
# get p for Very conservative:
1 / (1 + exp(1)^-(0.2 + (0.5*3)))
```
Moderate:
$$p = \dfrac {1}{1 + e^{-(0.2 + 0.5(0))}} = 0.55. $$
```{r}
## calculations
# get p for Moderate:
1 / (1 + exp(1)^-(0.2 + (0.5*0)))
```
Very Liberal:
$$p = \dfrac {1}{1 + e^{-(0.2 + 0.5(-3))}} = 0.21. $$
```{r}
## calculations
# get p for Very Liberal:
1 / (1 + exp(1)^-(0.2 + (0.5*(-3))))
```

## (e)

Predicted probabilities (?):

$$p' = \dfrac {d}{dx}  \bigg( \dfrac {1}{1 + e^{-(0.2 + 0.5x_i)}} \bigg). $$
Need chain rule...
$$p = \dfrac{1}{A}, \ \ A = 1 + e^B, \ \ B = -(0.2 + 0.5x_i). $$
$$p' = \dfrac {d}{dp} \bigg( \dfrac{1}{A} \bigg) = - \dfrac {1}{A^2}. $$
$$A' = \dfrac{d}{dA} (1 + e^B) = \dfrac{d}{dA}(1) + \dfrac{d}{dA}(e^B) = 0 + e^B = e^B. $$
$$ B' = \dfrac{d}{dB} [-(0.2 + 0.5x_i)] = \dfrac{d}{dB}(-0.2) + (-0.5)\dfrac{d}{dB}(x_i) = 0 - 0.5 \times 1 = -0.5. $$
$$\dfrac {(-1) (e^B)(-0.5)}{A^2} = \dfrac {(0.5)e^B}{A^2}, $$
$$\dfrac {(0.5)e^B}{A^2} = \dfrac {(0.5)e^B}{(1 + e^B)^2}, $$
$$\dfrac {(0.5)e^B}{(1 + e^B)^2} = \dfrac {0.5e^{-(0.2 + 0.5x_i)}} {(1 + e ^{-(0.2 + 0.5x_i)})^2}.$$
Plug in zero:
$$\dfrac {0.5e^{-(0.2 + 0.5(0))}} {(1 + e ^{-(0.2 + 0.5(0))})^2} = 0.124.$$
```{r}
# calculation
0.5 * exp(1)^-(0.2+(0.5 * 0)) / (1 + exp(1)^-(0.2+(0.5 * 0)))^2
```

## (f)

This funciton gives you the instantaneous rate of change for a given x value. This means that for a moderate voter (x = 0), the instantaneous rate of change in probability of voting for the incumbant is 0.124. In other words, it is the slope of the line at a given point of x. 

