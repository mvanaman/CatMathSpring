---
title: "Math HW5"
author: "Matthew Vanaman"
date: '`r format(Sys.Date(), "%m-%d-%Y")`'
monofont: Courier New
output:
  pdf_document: 
    latex_engine: xelatex
  word_document: default
header-includes: \usepackage{tikz,xcolor,listings}
---
`r knitr::opts_knit$set(root.dir='..')`

```{r, include=FALSE}
library(ProjectTemplate); load.project()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
options(scipen = 999)
knitr::opts_chunk$set(engine.path = "/anaconda3/bin/python")
np <- reticulate::import("numpy")
sympy <- reticulate::import_from_path("sympy", path = "/anaconda3/lib/python3.7/site-packages")
```

5.2, 5.3

# 4.8
## (a)



### Work 

#### First Derivative:
The only layer here is in the exponent of $e$; everything else is constant.
$$f (x) = \dfrac {1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}, $$
$$A = -\frac {x^2}{2}, \ \ f(x) = \dfrac {1} {\sqrt{2 \pi}} e^A $$
$$A' = \dfrac {d}{dx} \Big( -\dfrac{x^2}{2} \Big) \\ = - \dfrac {1}{2} \Big(2x^{2-1}) \\ = - \dfrac {2x^{2-1}}{2} \\ = -x. $$
$$ \dfrac {1} {\sqrt{2 \pi}} e^A \Big(-x \Big) = \dfrac {-x} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}. $$

#### Second Derivative

Because the first derivative was multiplicative, we need the product to get the second derivative. \newline
Product rule: $f''(x) = g'(x) f(x) + g(x) f'(x).$ \newline
$f(x) =  \frac {1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}.$ \newline
$f'(x) = \frac {-x} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}.$ \newline
$g(x) = -x.$\newline
$g'(x) = \frac {d}{dx}(-x) = -1 .$

$$f''(x) =  - 1 \bigg( \dfrac {1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}} \bigg) - x \bigg( \dfrac {-x^2} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}\bigg)$$ 
$$= -\bigg( \dfrac {1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}} \bigg) +  \bigg( \dfrac {x^2} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}\bigg)$$
$$= \bigg( \dfrac {x^2 - 1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}\bigg).$$

#### Plug in 0 for each version

$$f(0) = \dfrac {1} {\sqrt{2 \pi}} e^{-\frac {0^2}{2}} = \dfrac {1}{\sqrt{2 \pi}}. $$
$$f'(0) = \dfrac {0^2} {\sqrt{2 \pi}} e^{-\frac {0^2}{2}} = 0. $$
$$f''(0) = \dfrac {0^2 - 1} {\sqrt{2 \pi}} e^{-\frac {0^2}{2}} = \dfrac {-1} {\sqrt{2 \pi}}. $$\newline
Taylor approximation - just plug in what we found:
$$f(x) \approx \dfrac {1}{\sqrt{2 \pi}} + 0x + \dfrac {-1 / \sqrt{2 \pi}} {2}x^2.$$
$$f''(x) \approx \dfrac {-1}{\sqrt{8 \pi}}x^2 + \dfrac {1} {\sqrt{2 \pi}}. $$

## (b)

```{r, echo=FALSE}
x <- seq(-3, 3, length=1000)
norm <- dnorm(x, mean=0, sd=1)
taylor <- (-1/sqrt(8*c(pi)))*x^2 + (1/sqrt(2*c(pi)))
df <- as.data.frame(cbind(x,norm,taylor))
ggplot(df, aes(x)) +
  geom_line(aes(y = norm, color = "Normal")) +
  geom_line(aes(y = taylor, color = "Taylor\nApproximation")) +
  ylab("") + xlab("") +
  labs(color = "Function") +
  theme_minimal()
```

Taylor series seems most accurate between about -1 and 1.


# 4.9
## (a)

Start with
$$\ln \bigg(\prod_{i = 1}^{N} p^{y_{i}} (1 - p)^{1 - y_{i}} \bigg), $$
$$\sum_{i = 1}^{N} \ln \bigg( p^{y_{i}} (1 - p)^{1-y_i} \bigg), $$
$$\sum_{i = 1}^{N} \bigg( \ln \big( p^{y_{i}} \big) + \ln\big( (1 - p)^{1-y_i} \big) \bigg), $$
$$\sum_{i = 1}^{N} \bigg( (y_i) \ln \big(p \big) + \ln\big( (1 - p)^{1-y_i} \big) \bigg), $$
$$\sum_{i = 1}^{N} \bigg( (y_i) \ln \big(p \big) + (1-y_i) \ln (1 - p) \bigg), $$
$$\sum_{i = 1}^{N} (y_i) \ln \big(p \big) + \sum_{i = 1}^{N}(1-y_i) \ln (1 - p), $$
$$\ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \sum_{i = 1}^{N}(1-y_i), $$
$$\ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \bigg(\sum_{i = 1}^{N}1\bigg)- \bigg(\sum_{i = 1}^{N}y_i\bigg), $$
$$\ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \bigg(N - \sum_{i = 1}^{N}y_i\bigg). $$


## (b)

$$\dfrac {d}{dp} \Bigg( \ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \bigg(N - \sum_{i = 1}^{N}y_i\bigg) \Bigg), $$
$$\dfrac {d}{dp} \Bigg( \ln \big(p \big) \sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {d}{dp} \Bigg( \ln (1 - p) \bigg(N - \sum_{i = 1}^{N}y_i\bigg) \Bigg), $$
$$\dfrac {d}{dp} \Bigg( \ln \big(p \big) \Bigg) \dfrac {d}{dp} \Bigg(\sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {d}{dp} \Bigg( \ln (1 - p)\Bigg) \dfrac {d}{dp} \Bigg(N - \sum_{i = 1}^{N}y_i \Bigg), $$
$$\dfrac {d}{dp} \Bigg( \ln \big(p \big) \Bigg) \dfrac {d}{dp} \Bigg(\sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {d}{dp} \Bigg( \ln (1 - p)\Bigg) \dfrac {d}{dp} \Bigg(N - \sum_{i = 1}^{N}y_i \Bigg), $$
$$\dfrac {1}{p} \Bigg(\sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {d}{dp} \Bigg( \ln (1 - p)\Bigg) \Bigg(N - \sum_{i = 1}^{N}y_i \Bigg). $$
Gotta use the chain rule:
$$ \dfrac {d}{dp} \Bigg( \ln (1 - p)\Bigg) = \bigg(\dfrac {1}{1 - p}\bigg)\dfrac {d}{dp}(1 - p) = \dfrac{ \frac {d}{dp}(1) - \frac {d}{dp}(p)} {1 - p} = \dfrac {0-1}{1-p} = -\dfrac {-1}{1-p}. $$
$$\dfrac {1}{p} \Bigg(\sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {-1}{1- p} \Bigg(N - \sum_{i = 1}^{N}y_i \Bigg), $$
$$\dfrac {1}{p} \Bigg(\dfrac {\sum_{i = 1}^{N} (y_i)}{1} \Bigg) + \dfrac {-1}{1- p} \Bigg(\dfrac {N - \sum_{i = 1}^{N}y_i}{1} \Bigg), $$
$$\dfrac {\sum_{i = 1}^{N} (y_i)}{p} - \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}. $$

## (c)

$$0 = \dfrac {\sum_{i = 1}^{N} (y_i)}{p} - \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}, $$
$$\dfrac {\sum_{i = 1}^{N} (y_i)}{p} = \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}, $$
$$\dfrac {\sum_{i = 1}^{N} (y_i)}{p} = \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}, $$
$$ \sum_{i = 1}^{N} y_i(1 -p) = p \bigg(N - \sum_{i = 1}^{N}y_i \bigg), $$
$$ \sum_{i = 1}^{N} y_i - \sum_{i = 1}^{N} y_i (p) = (p)N - (p)\sum_{i = 1}^{N}y_i, $$
$$ \sum_{i = 1}^{N} y_i = (p)N, $$
$$\dfrac {\sum_{i = 1}^{N} y_i}{N} = p. $$
In the maximum likelihood framework, the proportion observed in the data tracks the probability of in the population, assuming assumptions hold. In this case, we have a sample where we add up all of the y values, the divide by the number of values. This gives us the sample proportion (or mean), which is an estimate of the probability.

## (d)

If $ p = \frac {\sum_{i = 1}^{N} y_i}{N} $, and we're given a function that gives us p, then all we have to do is substitute the function for p, then substitute $x_i$ into that function.
$$p = \dfrac {1}{1 + e^{-(0.2 + 0.5x_i)}}. $$
Very Conservative:
$$p = \dfrac {1}{1 + e^{-(0.2 + 0.53)}} = 0.85. $$
```{r}
## calculations
# get euler's number
exp(1)
# get p for Very conservative:
1 / (1 + exp(1)^-(0.2 + (0.5*3)))
```
Moderate:
$$p = \dfrac {1}{1 + e^{-(0.2 + 0.5(0))}} = 0.55. $$
```{r}
## calculations
# get p for Moderate:
1 / (1 + exp(1)^-(0.2 + (0.5*0)))
```
Very Liberal:
$$p = \dfrac {1}{1 + e^{-(0.2 + 0.5(-3))}} = 0.21. $$
```{r}
## calculations
# get p for Very Liberal:
1 / (1 + exp(1)^-(0.2 + (0.5*(-3))))
```

## (e)

Predicted probabilities (?):

$$p' = \dfrac {d}{dx}  \bigg( \dfrac {1}{1 + e^{-(0.2 + 0.5x_i)}} \bigg). $$
Need chain rule...
$$p = \dfrac{1}{A}, \ \ A = 1 + e^B, \ \ B = -(0.2 + 0.5x_i). $$
$$p' = \dfrac {d}{dp} \bigg( \dfrac{1}{A} \bigg) = - \dfrac {1}{A^2}. $$
$$A' = \dfrac{d}{dA} (1 + e^B) = \dfrac{d}{dA}(1) + \dfrac{d}{dA}(e^B) = 0 + e^B = e^B. $$
$$ B' = \dfrac{d}{dB} [-(0.2 + 0.5x_i)] = \dfrac{d}{dB}(-0.2) + (-0.5)\dfrac{d}{dB}(x_i) = 0 - 0.5 \times 1 = -0.5. $$
$$\dfrac {(-1) (e^B)(-0.5)}{A^2} = \dfrac {(0.5)e^B}{A^2}, $$
$$\dfrac {(0.5)e^B}{A^2} = \dfrac {(0.5)e^B}{(1 + e^B)^2}, $$
$$\dfrac {(0.5)e^B}{(1 + e^B)^2} = \dfrac {0.5e^{-(0.2 + 0.5x_i)}} {(1 + e ^{-(0.2 + 0.5x_i)})^2}.$$
Plug in zero:
$$\dfrac {0.5e^{-(0.2 + 0.5(0))}} {(1 + e ^{-(0.2 + 0.5(0))})^2} = 0.124.$$
```{r}
# calculation
0.5 * exp(1)^-(0.2+(0.5 * 0)) / (1 + exp(1)^-(0.2+(0.5 * 0)))^2
```

## (f)

This funciton gives you the instantaneous rate of change for a given x value. This means that for a moderate voter (x = 0), the instantaneous rate of change in probability of voting for the incumbant is 0.124. In other words, it is the slope of the line at a given point of x. 


# 5.1

## (a)


Take derivative:
$$f(x) = 3x^4 - 4x^3 - 36x^2, $$
$$f'(x) = (4)3x^{4 -1} - (3)4x^{3-1} - (2)36x^{2-1} = 12x^3 - 12x^2 - 72x.$$
$$0 = 12x^3 - 12x^2 - 72x$$
$$= 12(x^3 - x^2 - 6x)$$
$$= 12x(x^2 - x - 6)$$
$$= 12x(x - 3)(x + 2).$$
Critical points are when x equals 3, -2, or 0. Using second derivitive test, use the critical points rules:
$$f''(x) = (3)12x^{3-1} - (2)12x^{2-1} - 72x^{1-1} = 36x^2 - 24x - 72. $$
For x = -2:
$$36(-2)^2 - 24(-2) - 72 = 120 $$
```{r}
# calculation
36 * (-2)^2 - 24 * (-2) - 72
```
For x = 3
$$36(3)^2 - 24(3) - 72 = 180$$
```{r}
36*3^2 - 24 * 3 - 72
```
For x = 0:
$$36(0)^2 - 24(0) - 72 = -72$$
```{r}
36*0^2 - 24 * 0 - 72
```
According to the second derivative test, a critical point is a local maximum if $f''(x) < 0$, a local minimum if $f''(x) > 0$, and is a saffle point when $f''(x) = 0$. This means critical points x = 3 and -2 are local minimums while x = 0 is a local maximum. \newline
to find global max and min, first plug in the boundary points into the original function.\newline
Lower boundary:
$$f(-4) = 3(-4)^4 - 4(-4)^3 - 36(-4)^2 = 448$$
```{r}
# calculation
3*(-4)^4 - 4*(-4)^3 - 36*(-4)^2
```
Upper boundary:
$$f(4) = 3(4)^4 - 4(4)^3 - 36(4)^2 = -64$$
```{r}
# calculation
3*(4)^4 - 4*(4)^3 - 36*(4)^2
```
Finally, compare these to the local minimum and maximum points:\newline
local maximum: 
$$f(0) = 3(0)^4 - 4(0)^3 - 36(0)^2 = 0. $$
```{r}
3*(0)^4 - 4*(0)^3 - 36*(0)^2
```

Local minima:
$$f(3) = 3(3)^4 - 4(3)^3 - 36(3)^2 = -189. $$
$$f(-2) = 3(-2)^4 - 4(-2)^3 - 36(-2)^2 = -64. $$
```{r}
3*(3)^4 - 4*(3)^3 - 36*(3)^2
```
```{r}
3*(-2)^4 - 4*(-2)^3 - 36*(-2)^2
```
Output of function is lowest at x = (3), therefore this is the global minimum. Output of function is highest at x = -4, therefore this is global maximum. 



## (b)
*lower boundary approaches 0.

$$g'(x) = \dfrac {d}{dx} (x \ln (x) - x),$$
$$= \dfrac {d}{dx} \big(x \ln (x)\big) - \dfrac {d}{dx}(x),$$
$$= (x) \dfrac {d}{dx} \ln (x) - 1,$$
$$= (x) \dfrac {1}{x} + \ln (x) - 1,$$
$$= 1 + \ln (x) - 1,$$
$$= \ln (x).$$
Set derivative equal to zero:
$$0 = \ln (x) \ \text {when} \ x = 1.$$
Is critical point positive or negative?
$$g''(x) = \dfrac {x}{dx} \Big(\ln(x)\Big) = \dfrac {1}{x}, $$
$$g''(x) = \dfrac {x}{dx} \Big(\ln(1)\Big) = \dfrac {1}{1} = 1. $$
Positve, therefore local minimum. Now compare critical point $x = 1$ with boundary point in original function:
$$f(1) = (1) \ln(1) - 1 = -1. $$
```{r}
1 * log(1) - 1
```
$$f(3) = (3) \ln(3) - 3 = 2.30 $$
```{r}
(3) * log(3) - 3
```
Critical point $x = 1$ is global minimum (-1) and $x = 3$ is global maximum (2.30). 


# 5.2

## (a)

