---
title: "Math HW5"
author: "Matthew Vanaman"
date: '`r format(Sys.Date(), "%m-%d-%Y")`'
monofont: Courier New
output:
  pdf_document: 
    latex_engine: xelatex
  word_document: default
header-includes: \usepackage{tikz,xcolor,listings}
---
`r knitr::opts_knit$set(root.dir='..')`

```{r, include=FALSE}
library(ProjectTemplate); load.project()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
options(scipen = 999)
knitr::opts_chunk$set(engine.path = "/anaconda3/bin/python")
np <- reticulate::import("numpy")
sympy <- reticulate::import_from_path("sympy", path = "/anaconda3/lib/python3.7/site-packages")
```

*All work and code are shown in the appendix.*

# 4.8

## (a)

**Answer:**\newline

$f''(x) \approx \dfrac {-1}{\sqrt{8 \pi}}x^2 + \dfrac {1} {\sqrt{2 \pi}}.$

## (b)
**Answer:**\newline
```{r, echo=FALSE}
x <- seq(-3, 3, length=1000)
norm <- dnorm(x, mean=0, sd=1)
taylor <- (-1/sqrt(8*c(pi)))*x^2 + (1/sqrt(2*c(pi)))
df <- as.data.frame(cbind(x,norm,taylor))
ggplot(df, aes(x)) +
  geom_line(aes(y = norm, color = "Normal")) +
  geom_line(aes(y = taylor, color = "Taylor\nApproximation")) +
  ylab("") + xlab("") +
  labs(color = "Function") +
  theme_minimal()
```

Taylor series seems most accurate between about -1 and 1.

# 4.9
## (a)
**Answer:**\newline
Start with
$$\ln \bigg(\prod_{i = 1}^{N} p^{y_{i}} (1 - p)^{1 - y_{i}} \bigg), $$
$$\sum_{i = 1}^{N} \ln \bigg( p^{y_{i}} (1 - p)^{1-y_i} \bigg), $$
$$\sum_{i = 1}^{N} \bigg( \ln \big( p^{y_{i}} \big) + \ln\big( (1 - p)^{1-y_i} \big) \bigg), $$
$$\sum_{i = 1}^{N} \bigg( (y_i) \ln \big(p \big) + \ln\big( (1 - p)^{1-y_i} \big) \bigg), $$
$$\sum_{i = 1}^{N} \bigg( (y_i) \ln \big(p \big) + (1-y_i) \ln (1 - p) \bigg), $$
$$\sum_{i = 1}^{N} (y_i) \ln \big(p \big) + \sum_{i = 1}^{N}(1-y_i) \ln (1 - p), $$
$$\ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \sum_{i = 1}^{N}(1-y_i), $$
$$\ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \bigg(\sum_{i = 1}^{N}1\bigg)- \bigg(\sum_{i = 1}^{N}y_i\bigg),$$
$$\ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \bigg(N - \sum_{i = 1}^{N}y_i\bigg).$$


## (b)
**Answer:**\newline
$\dfrac {\sum_{i = 1}^{N} (y_i)}{p} - \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}.$

## (c)
**Answer:**\newline
$\dfrac {\sum_{i = 1}^{N} y_i}{N} = p.$\newline
\newline
In the maximum likelihood framework, the proportion observed in the data tracks the probability of in the population, assuming assumptions hold. In this case, we have a sample where we add up all of the y values, the divide by the number of values. This gives us the sample proportion (or mean), which is an estimate of the probability.

## (d)
**Answer:**\newline
Very conservative: 0.85\newline
Moderate: 0.55\newline
Very liberal: 0.21\newline

## (e)
**Answer:**\newline
Plug in zero: \newline
$\dfrac {0.5e^{-(0.2 + 0.5(0))}} {(1 + e ^{-(0.2 + 0.5(0))})^2} = 0.124.$

## (f)
**Answer:**\newline
This funciton gives you the instantaneous rate of change for a given x value. This means that for a moderate voter (x = 0), the instantaneous rate of change in probability of voting for the incumbant is 0.124. In other words, it is the slope of the line at a given point of x. 

# 5.1

## (a)
**Answer:**\newline
Output of function is lowest at x = 3 (-189), therefore this is the global minimum. Output of function is highest at x = -4 (448), therefore this is global maximum. 

## (b)
**Answer:**\newline
Critical point $x = 1$ is global minimum (-1) and $x = 3$ is global maximum (2.30). 

# 5.2

## (a)
**Answer:**\newline
Critical points x = 4 and x = 6 are the locations of the global minimum and maximum, respectively. 

## (b)
**Answer:**\newline
Starting at 2:

```{r, echo=FALSE}
# make some functions
f_p <- function (x) {3*x^2 - 15*x + 12}
f_pp <- function (x) {6*x - 15}
f_nr <- function (x) {x - (3*x^2 - 15*x + 12) / (6*x - 15)}
# make a matrix with iteration 0
iter_0 <- matrix(c(0, 2, f_p(2), f_pp(2), f_nr(2)), nrow = 1, ncol=5)
colnames(iter_0) <- c("Iteration", "x", "fp(x)", "fpp(x)", "fNR")
# see what iteration 0 was, plug into functions
iter_1 <- rbind(iter_0, matrix(c(1, f_nr(2), f_p(f_nr(2)), f_pp(f_nr(2)), f_nr(f_nr(2))), nrow = 1, ncol=5))
# see what iteration 1 was, etc...
iter_2 <- rbind(iter_1, matrix(c(2, 0.8, f_p(0.8), f_pp(0.8), f_nr(0.8)), nrow = 1, ncol=5))
iter_3 <- rbind(iter_2, matrix(c(3, 0.9882353, f_p(0.9882353), f_pp(0.9882353), f_nr(0.9882353)), nrow = 1, ncol=5))
iter_4 <- rbind(iter_3, matrix(c(4, 0.9999542, f_p(0.9999542), f_pp(0.9999542), f_nr(0.9999542)), nrow = 1, ncol=5))
iter_5 <- rbind(iter_4, matrix(c(5, 1, f_p(1), f_pp(1), f_nr(1)), nrow = 1, ncol=5))
# Probably an easier way to do this, but gets the job done...
knitr::kable(iter_5)
```

Converges after 5, at root 1.

Starting at 5:
```{r, echo=FALSE}
# make a matrix with iteration 0
iter_0 <- matrix(c(0, 5, f_p(5), f_pp(5), f_nr(5)), nrow = 1, ncol=5)
colnames(iter_0) <- c("Iteration", "x", "fp(x)", "fpp(x)", "fNR")
# see what iteration 0 was, plug into functions
iter_1 <- rbind(iter_0, matrix(c(1, f_nr(5), f_p(f_nr(5)), f_pp(f_nr(5)), f_nr(f_nr(5))), nrow = 1, ncol=5))
# see what iteration 1 was, etc...
iter_2 <- rbind(iter_1, matrix(c(2, 4.011765, f_p(4.011765), f_pp(4.011765), f_nr(4.011765)), nrow = 1, ncol=5))

iter_3 <- rbind(iter_2, matrix(c(3, 4.000046, f_p(4.000046), f_pp(4.000046), f_nr(4.000046)), nrow = 1, ncol=5))

iter_4 <- rbind(iter_3, matrix(c(4, 4, f_p(4), f_pp(4), f_nr(4)), nrow = 1, ncol=5))
# Meh, only took 5 seconds this time
knitr::kable(iter_4)
```

Converges after 4, at root 4.

## (c)
**Answer:**\newline
One problem is that there could be more than one root. If you don't know this, you may have missed out on another convergence point and come to the wrong conclusion that the convergence point was the only root, or the best root (on substantive grounds). Unfortuntaly there's no way to tell unless you try out different starting values. \newline

Another problem is that the convergence point is itself limited in the information it gives you. If there are multiple convergence points, that could speak to the type of curve that function produces e.g. a cubic function has a local maximum and global maximum, but you don't necessarily know whether the convergence point arrived at the local or global maximum. You could potentially misunderstand what the function is telling you. 

# 5.3

## (a)
**Answer:**\newline
Repubs should wait 11.76 days before reactivating the government.

**Demonstrate that answer represents the global maximum:** \newline
You could plug numbers into the function and get close the maximizing point, but that would be time consuming and would give a imprecise answer.\newline
Istead, we could try to find the global maximum. \newline
Take the first derivative:
$$f'(t) = \dfrac {d}{dt} (30 \ln (t + 1)) - \dfrac {d}{dt} \bigg(\dfrac {t^2}{10}\bigg), $$
$$f'(t) = (30) \dfrac {1}{t + 1} \bigg(\dfrac {d}{dt}\bigg) \big(t + 1\big) - \dfrac {2t}{10}, $$
$$f'(t) = \dfrac {30}{t + 1} - \dfrac {t}{5}. $$
Get the critical points:
$$0 = \dfrac {30}{t + 1} - \dfrac {t}{5}, $$
$$ \dfrac {30}{t + 1} = \dfrac {t}{5},$$
$$ \dfrac {150}{t + 1} = t,$$
$$ 150 = t(t + 1),$$
$$ 0 = t^2 + t - 150.$$
If Ax^2 + Bx + C = 0, then:\newline 
$$x = \frac {- Bx \pm \sqrt{B^2 - 4AC}} {2A}.$$
Plugging in, we get the critical points of x:
```{r}
(-1 + sqrt((1^2) - 4 * 1 * - 150)) / (2 * 1)
(-1 - sqrt(1^2 - (4 * 1) * (- 150))) / (2 * 1)
```
If t standas for number of days, the negative critical point doesn't help us make a decision. But does 11.75 represent a local or global maximum? To the left:
```{r}
(30 / (10.76 + 1)) - (10.76 / 5)
```
Positive. \newline
To the right:
```{r}
(30 / (12.76 + 1)) - (12.76 / 5)
```
Negative. Therefore, using the efirst derivative test, 11.76 is a maximum. All we have left to compare that to is 0 plugged into the original function:
```{r}
30 * log(0 + 1) - (0^2 / 10)
```
Which makes sense; Repubs can't benefit after zero time has passed. Because the only two points to consider are 11.76 and 0, 11.76 must therefore be the global maximum. Repubs should wait 11.76 days before reactivating the government. \newpage

\begin{center}
\newpage
\LARGE\bfseries Appendix
\end{center}

# 4.8
## (a)

### Work 

#### First Derivative:
The only layer here is in the exponent of $e$; everything else is constant.
$$f (x) = \dfrac {1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}, $$
$$A = -\frac {x^2}{2}, \ \ f(x) = \dfrac {1} {\sqrt{2 \pi}} e^A,$$
$$A' = \dfrac {d}{dx} \Big( -\dfrac{x^2}{2} \Big) \\ = - \dfrac {1}{2} \Big(2x^{2-1}) \\ = - \dfrac {2x^{2-1}}{2} \\ = -x.$$
$$\dfrac {1} {\sqrt{2 \pi}} e^A \Big(-x \Big) = \dfrac {-x} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}.$$

#### Second Derivative

Because the first derivative was multiplicative, we need the product to get the second derivative. \newline
Product rule: $f''(x) = g'(x) f(x) + g(x) f'(x).$ \newline
$f(x) =  \frac {1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}.$ \newline
$f'(x) = \frac {-x} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}.$ \newline
$g(x) = -x.$\newline
$g'(x) = \frac {d}{dx}(-x) = -1 .$

$$f''(x) =  - 1 \bigg( \dfrac {1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}} \bigg) - x \bigg( \dfrac {-x^2} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}\bigg)$$ 
$$= -\bigg( \dfrac {1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}} \bigg) +  \bigg( \dfrac {x^2} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}\bigg)$$
$$= \bigg( \dfrac {x^2 - 1} {\sqrt{2 \pi}} e^{-\frac {x^2}{2}}\bigg).$$

#### Plug in 0 for each version

$$f(0) = \dfrac {1} {\sqrt{2 \pi}} e^{-\frac {0^2}{2}} = \dfrac {1}{\sqrt{2 \pi}}. $$
$$f'(0) = \dfrac {0^2} {\sqrt{2 \pi}} e^{-\frac {0^2}{2}} = 0.$$
$$f''(0) = \dfrac {0^2 - 1} {\sqrt{2 \pi}} e^{-\frac {0^2}{2}} = \dfrac {-1} {\sqrt{2 \pi}}.$$\newline
Taylor approximation - just plug in what we found:
$$f(x) \approx \dfrac {1}{\sqrt{2 \pi}} + 0x + \dfrac {-1 / \sqrt{2 \pi}} {2}x^2.$$
$$f''(x) \approx \dfrac {-1}{\sqrt{8 \pi}}x^2 + \dfrac {1} {\sqrt{2 \pi}}.$$

## (b)
**Work:**
```{r, results = "hide", fig.show="hide"}
x <- seq(-3, 3, length=1000)
norm <- dnorm(x, mean=0, sd=1)
taylor <- (-1/sqrt(8*c(pi)))*x^2 + (1/sqrt(2*c(pi)))
df <- as.data.frame(cbind(x,norm,taylor))
ggplot(df, aes(x)) +
  geom_line(aes(y = norm, color = "Normal")) +
  geom_line(aes(y = taylor, color = "Taylor\nApproximation")) +
  ylab("") + xlab("") +
  labs(color = "Function") +
  theme_minimal()
```

Taylor series seems most accurate between about -1 and 1.


# 4.9
## (a)
**Work:** \newline
Start with
$$\ln \bigg(\prod_{i = 1}^{N} p^{y_{i}} (1 - p)^{1 - y_{i}} \bigg), $$
$$\sum_{i = 1}^{N} \ln \bigg( p^{y_{i}} (1 - p)^{1-y_i} \bigg), $$
$$\sum_{i = 1}^{N} \bigg( \ln \big( p^{y_{i}} \big) + \ln\big( (1 - p)^{1-y_i} \big) \bigg), $$
$$\sum_{i = 1}^{N} \bigg( (y_i) \ln \big(p \big) + \ln\big( (1 - p)^{1-y_i} \big) \bigg), $$
$$\sum_{i = 1}^{N} \bigg( (y_i) \ln \big(p \big) + (1-y_i) \ln (1 - p) \bigg), $$
$$\sum_{i = 1}^{N} (y_i) \ln \big(p \big) + \sum_{i = 1}^{N}(1-y_i) \ln (1 - p), $$
$$\ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \sum_{i = 1}^{N}(1-y_i), $$
$$\ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \bigg(\sum_{i = 1}^{N}1\bigg)- \bigg(\sum_{i = 1}^{N}y_i\bigg), $$
$$\ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \bigg(N - \sum_{i = 1}^{N}y_i\bigg). $$


## (b)
**Work:** 
$$\dfrac {d}{dp} \Bigg( \ln \big(p \big) \sum_{i = 1}^{N} (y_i) +  \ln (1 - p) \bigg(N - \sum_{i = 1}^{N}y_i\bigg) \Bigg), $$
$$\dfrac {d}{dp} \Bigg( \ln \big(p \big) \sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {d}{dp} \Bigg( \ln (1 - p) \bigg(N - \sum_{i = 1}^{N}y_i\bigg) \Bigg), $$
$$\dfrac {d}{dp} \Bigg( \ln \big(p \big) \Bigg) \dfrac {d}{dp} \Bigg(\sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {d}{dp} \Bigg( \ln (1 - p)\Bigg) \dfrac {d}{dp} \Bigg(N - \sum_{i = 1}^{N}y_i \Bigg), $$
$$\dfrac {d}{dp} \Bigg( \ln \big(p \big) \Bigg) \dfrac {d}{dp} \Bigg(\sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {d}{dp} \Bigg( \ln (1 - p)\Bigg) \dfrac {d}{dp} \Bigg(N - \sum_{i = 1}^{N}y_i \Bigg), $$
$$\dfrac {1}{p} \Bigg(\sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {d}{dp} \Bigg( \ln (1 - p)\Bigg) \Bigg(N - \sum_{i = 1}^{N}y_i \Bigg). $$
Gotta use the chain rule:
$$ \dfrac {d}{dp} \Bigg( \ln (1 - p)\Bigg)$$ 
$$= \bigg(\dfrac {1}{1 - p}\bigg)\dfrac {d}{dp}(1 - p),$$ 
$$= \dfrac{ \frac {d}{dp}(1) - \frac {d}{dp}(p)} {1 - p} = \dfrac {0-1}{1-p} = -\dfrac {-1}{1-p}.$$
$$\dfrac {1}{p} \Bigg(\sum_{i = 1}^{N} (y_i) \Bigg) + \dfrac {-1}{1- p} \Bigg(N - \sum_{i = 1}^{N}y_i \Bigg), $$
$$\dfrac {1}{p} \Bigg(\dfrac {\sum_{i = 1}^{N} (y_i)}{1} \Bigg) + \dfrac {-1}{1- p} \Bigg(\dfrac {N - \sum_{i = 1}^{N}y_i}{1} \Bigg), $$
$$\dfrac {\sum_{i = 1}^{N} (y_i)}{p} - \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}. $$

## (c)
**Work:** 
$$0 = \dfrac {\sum_{i = 1}^{N} (y_i)}{p} - \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}, $$
$$\dfrac {\sum_{i = 1}^{N} (y_i)}{p} = \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}, $$
$$\dfrac {\sum_{i = 1}^{N} (y_i)}{p} = \dfrac {N - \sum_{i = 1}^{N}y_i}{1 - p}, $$
$$ \sum_{i = 1}^{N} y_i(1 -p) = p \bigg(N - \sum_{i = 1}^{N}y_i \bigg), $$
$$ \sum_{i = 1}^{N} y_i - \sum_{i = 1}^{N} y_i (p) = (p)N - (p)\sum_{i = 1}^{N}y_i, $$
$$ \sum_{i = 1}^{N} y_i = (p)N, $$
$$\dfrac {\sum_{i = 1}^{N} y_i}{N} = p. $$
In the maximum likelihood framework, the proportion observed in the data tracks the probability of in the population, assuming assumptions hold. In this case, we have a sample where we add up all of the y values, the divide by the number of values. This gives us the sample proportion (or mean), which is an estimate of the probability.

## (d)
**Work:** \newline
If $p = \frac {\sum_{i = 1}^{N} y_i}{N}$, and we're given a function that gives us p, then all we have to do is substitute the function for p, then substitute $x_i$ into that function.
$$p = \dfrac {1}{1 + e^{-(0.2 + 0.5x_i)}}. $$
Very Conservative:
$$p = \dfrac {1}{1 + e^{-(0.2 + 0.53)}} = 0.85. $$
```{r}
## calculations
# get euler's number
exp(1)
# get p for Very conservative:
1 / (1 + exp(1)^-(0.2 + (0.5*3)))
```
Moderate:
$$p = \dfrac {1}{1 + e^{-(0.2 + 0.5(0))}} = 0.55. $$
```{r}
## calculations
# get p for Moderate:
1 / (1 + exp(1)^-(0.2 + (0.5*0)))
```
Very Liberal:
$$p = \dfrac {1}{1 + e^{-(0.2 + 0.5(-3))}} = 0.21. $$
```{r}
## calculations
# get p for Very Liberal:
1 / (1 + exp(1)^-(0.2 + (0.5*(-3))))
```

## (e)
**Work:** \newline
$$p' = \dfrac {d}{dx}  \bigg( \dfrac {1}{1 + e^{-(0.2 + 0.5x_i)}} \bigg). $$
Need chain rule...
$$p = \dfrac{1}{A}, \ \ A = 1 + e^B, \ \ B = -(0.2 + 0.5x_i). $$
$$p' = \dfrac {d}{dp} \bigg( \dfrac{1}{A} \bigg) = - \dfrac {1}{A^2}. $$
$$A' = \dfrac{d}{dA} (1 + e^B) = \dfrac{d}{dA}(1) + \dfrac{d}{dA}(e^B) = 0 + e^B = e^B. $$
$$ B' = \dfrac{d}{dB} [-(0.2 + 0.5x_i)],$$ 
$$= \dfrac{d}{dB}(-0.2) + (-0.5)\dfrac{d}{dB}(x_i),$$,
$$= 0 - 0.5 \times 1 = -0.5.$$
$$\dfrac {(-1) (e^B)(-0.5)}{A^2} = \dfrac {(0.5)e^B}{A^2}, $$
$$\dfrac {(0.5)e^B}{A^2} = \dfrac {(0.5)e^B}{(1 + e^B)^2}, $$
$$\dfrac {(0.5)e^B}{(1 + e^B)^2} = \dfrac {0.5e^{-(0.2 + 0.5x_i)}} {(1 + e ^{-(0.2 + 0.5x_i)})^2}.$$
Plug in zero:
$$\dfrac {0.5e^{-(0.2 + 0.5(0))}} {(1 + e ^{-(0.2 + 0.5(0))})^2} = 0.124.$$
```{r}
# calculation
0.5 * exp(1)^-(0.2+(0.5 * 0)) / (1 + exp(1)^-(0.2+(0.5 * 0)))^2
```

## (f)
**Work:** \newline
N/A


# 5.1

## (a)
**Work:** \newline
Take derivative:
$$f(x) = 3x^4 - 4x^3 - 36x^2, $$
$$f'(x) = (4)3x^{4 -1} - (3)4x^{3-1} - (2)36x^{2-1} = 12x^3 - 12x^2 - 72x.$$
$$0 = 12x^3 - 12x^2 - 72x$$
$$= 12(x^3 - x^2 - 6x)$$
$$= 12x(x^2 - x - 6)$$
$$= 12x(x - 3)(x + 2).$$
Critical points are when x equals 3, -2, or 0. Using second derivitive test, use the critical points rules:
$$f''(x) = (3)12x^{3-1} - (2)12x^{2-1} - 72x^{1-1},$$ 
$$= 36x^2 - 24x - 72. $$
For x = -2:
$$36(-2)^2 - 24(-2) - 72 = 120 $$
```{r}
# calculation
36 * (-2)^2 - 24 * (-2) - 72
```
For x = 3
$$36(3)^2 - 24(3) - 72 = 180$$
```{r}
36*3^2 - 24 * 3 - 72
```
For x = 0:
$$36(0)^2 - 24(0) - 72 = -72$$
```{r}
36*0^2 - 24 * 0 - 72
```
According to the second derivative test, a critical point is a local maximum if $f''(x) < 0$, a local minimum if $f''(x) > 0$, and is a saffle point when $f''(x) = 0$. This means critical points x = 3 and -2 are local minimums while x = 0 is a local maximum. \newline
to find global max and min, first plug in the boundary points into the original function.\newline
Lower boundary:
$$f(-4) = 3(-4)^4 - 4(-4)^3 - 36(-4)^2 = 448$$
```{r}
# calculation
3*(-4)^4 - 4*(-4)^3 - 36*(-4)^2
```
Upper boundary:
$$f(4) = 3(4)^4 - 4(4)^3 - 36(4)^2 = -64$$
```{r}
# calculation
3*(4)^4 - 4*(4)^3 - 36*(4)^2
```
Finally, compare these to the local minimum and maximum points:\newline
local maximum: 
$$f(0) = 3(0)^4 - 4(0)^3 - 36(0)^2 = 0. $$
```{r}
3*(0)^4 - 4*(0)^3 - 36*(0)^2
```

Local minima:
$$f(3) = 3(3)^4 - 4(3)^3 - 36(3)^2 = -189. $$
$$f(-2) = 3(-2)^4 - 4(-2)^3 - 36(-2)^2 = -64. $$
```{r}
3*(3)^4 - 4*(3)^3 - 36*(3)^2
```
```{r}
3*(-2)^4 - 4*(-2)^3 - 36*(-2)^2
```
Output of function is lowest at x = (3), therefore this is the global minimum. Output of function is highest at x = -4, therefore this is global maximum. 

## (b)
**Work:** \newline
*lower boundary approaches 0.

$$g'(x) = \dfrac {d}{dx} (x \ln (x) - x),$$
$$= \dfrac {d}{dx} \big(x \ln (x)\big) - \dfrac {d}{dx}(x),$$
$$= (x) \dfrac {d}{dx} \ln (x) - 1,$$
$$= (x) \dfrac {1}{x} + \ln (x) - 1,$$
$$= 1 + \ln (x) - 1,$$
$$= \ln (x).$$
Set derivative equal to zero:
$$0 = \ln (x) \ \text {when} \ x = 1.$$
Is critical point positive or negative?
$$g''(x) = \dfrac {x}{dx} \Big(\ln(x)\Big) = \dfrac {1}{x}, $$
$$g''(x) = \dfrac {x}{dx} \Big(\ln(1)\Big) = \dfrac {1}{1} = 1. $$
Positve, therefore local minimum. Now compare critical point $x = 1$ with boundary point in original function:
$$f(1) = (1) \ln(1) - 1 = -1. $$
```{r}
1 * log(1) - 1
```
$$f(3) = (3) \ln(3) - 3 = 2.30 $$
```{r}
(3) * log(3) - 3
```
Critical point $x = 1$ is global minimum (-1) and $x = 3$ is global maximum (2.30). 


# 5.2

## (a)
**Work:** \newline
Take the derivative:
$$f'(x) = 3x^2 - 15x + 12,$$
$$= 3(x^2 - 5 + 4),$$
$$= 3(x -1)(x - 4).$$
We have critical points 1 and 4.\newline
First derivative test: left of x is positive:
$$f'(0) = 3(0)^2 - 15(0) + 12 = 12 $$
```{r}
3*(0)^2 - 15*(0) + 12
```
Right of x is negative:
$$= 3(2)^2 - 15(2) + 12 = -6$$
```{r}
3*(2)^2 - 15*(2) + 12
```
Critical point x = 1 is a local maximum.\newline
Critical point 4, to the left is negative:
$$f'(3) = 3(3)^2 - 15(3) + 12 = -6 $$
```{r}
3*(3)^2 - 15*(3) + 12
```
And right is positive:
$$f'(5) = 3(5)^2 - 15(5) + 12 = 12 $$
```{r}
3*(5)^2 - 15*(5) + 12
```
Critical point x = 4 is a local minimum.\newline
Compare critical points against boundary points:
$$f(x) = x^3 - \dfrac {15}{2} x^2 + 12x + 8$$
At lower boundary:
$$f(0) = 0^3 - \dfrac {15}{2} 0^2 + 12(0) + 8 = 8.$$
```{r}
0^3 - {15}/{2} * 0^2 + 12*(0) + 8
```
At upper boundary:
$$f(6) = 6^3 - \dfrac {15}{2} 6^2 + 12(6) + 8 = 26.$$
```{r}
6^3 - {15}/{2} * 6^2 + 12*(6) + 8
```
At critical point x = 1:
$$f(1) = 1^3 - \dfrac {15}{2} 1^2 + 12(1) + 8 = 13.5.$$
```{r}
1^3 - {15}/{2} * 1^2 + 12*(1) + 8
```
At critical point x = 4:
$$f(4) = 4^3 - \dfrac {15}{2} 4^2 + 12(4) + 8 = 0.$$
```{r}
4^3 - {15}/{2} * 4^2 + 12*(4) + 8
```
Critical points x = 4 and x = 6 are the locations of the global minimum and maximum, respectively. 

## (b)
**Work:** \newline
Starting at 2:
```{r, results = "hide"}
# make some functions
f_p <- function (x) {3*x^2 - 15*x + 12}
f_pp <- function (x) {6*x - 15}
f_nr <- function (x) {x - (3*x^2 - 15*x + 12) / (6*x - 15)}
# make a matrix with iteration 0
iter_0 <- matrix(c(0, 2, f_p(2), f_pp(2), f_nr(2)), nrow = 1, ncol=5)
colnames(iter_0) <- c("Iteration", "x", "fp(x)", "fpp(x)", "fNR")
# see what iteration 0 was, plug into functions
iter_1 <- rbind(iter_0, matrix(c(1, f_nr(2), f_p(f_nr(2)), f_pp(f_nr(2)), 
                                 f_nr(f_nr(2))), 
                                 nrow = 1, ncol=5))
# see what iteration 1 was, etc...
iter_2 <- rbind(iter_1, matrix(c(2, 0.8, f_p(0.8), f_pp(0.8), f_nr(0.8)), 
                               nrow = 1, ncol=5))
iter_3 <- rbind(iter_2, matrix(c(3, 0.9882353, f_p(0.9882353), 
                                 f_pp(0.9882353), f_nr(0.9882353)), 
                                 nrow = 1, 
                                 ncol=5))
iter_4 <- rbind(iter_3, matrix(c(4, 0.9999542, f_p(0.9999542), 
                                 f_pp(0.9999542), f_nr(0.9999542)), 
                                 nrow = 1, ncol=5))
iter_5 <- rbind(iter_4, matrix(c(5, 1, f_p(1), f_pp(1), f_nr(1)), 
                                nrow = 1, ncol=5))
# Probably an easier way to do this, but gets the job done...
knitr::kable(iter_5)
```

Converges after 5, at root 1.

Starting at 5:
```{r, results = "hide"}
# make a matrix with iteration 0
iter_0 <- matrix(c(0, 5, f_p(5), f_pp(5), f_nr(5)), 
                 nrow = 1, ncol=5)
colnames(iter_0) <- c("Iteration", "x", "fp(x)", "fpp(x)", "fNR")
# see what iteration 0 was, plug into functions
iter_1 <- rbind(iter_0, matrix(c(1, f_nr(5), f_p(f_nr(5)), 
                                 f_pp(f_nr(5)), f_nr(f_nr(5))), 
                                 nrow = 1, ncol=5))
# see what iteration 1 was, etc...
iter_2 <- rbind(iter_1, matrix(c(2, 4.011765, f_p(4.011765), 
                                 f_pp(4.011765), f_nr(4.011765)), 
                                 nrow = 1, ncol=5))
iter_3 <- rbind(iter_2, matrix(c(3, 4.000046, f_p(4.000046), 
                                 f_pp(4.000046), f_nr(4.000046)), 
                                 nrow = 1, ncol=5))
iter_4 <- rbind(iter_3, matrix(c(4, 4, f_p(4), f_pp(4), 
                                 f_nr(4)), 
                                 nrow = 1, ncol=5))
# Meh, only took 5 seconds this time
knitr::kable(iter_4)
```

Converges after 4, at root 4.

## (c)
**Work:** \newline
N/A

# 5.3

## (a)
**Work:** \newline
N/A (shown in answer)
