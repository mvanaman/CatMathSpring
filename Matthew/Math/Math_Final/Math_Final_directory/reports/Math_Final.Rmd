---
title: "Math Final"
author: "Matthew Vanaman"
date: '`r format(Sys.Date(), "%m-%d-%Y")`'
monofont: Courier New
figsintext: yes
output:
  pdf_document: 
    latex_engine: xelatex
  word_document: default
header-includes: \usepackage{tikz,xcolor,listings}
---
`r knitr::opts_knit$set(root.dir='..')`

```{r, include=FALSE}
library(ProjectTemplate); load.project()
library(papaja)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
options(scipen = 999, xtable.comment = FALSE)
knitr::opts_chunk$set(engine.path = "/anaconda3/bin/python")
```


# 7.11

## (a)

$$\frac {\partial} {\partial x_1} = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \beta_4 x_{i2}^3 + \beta_5 x_{i3} + \beta_6 x_{i4} + \beta_7 x_{i3} x_{i4},$$
Derivative of a constant is zero, so drop $\alpha$:
$$= \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \beta_4 x_{i2}^3 + \beta_5 x_{i3} + \beta_6 x_{i4} + \beta_7 x_{i3} x_{i4},$$
Break up across addition and subtraction, then bring out constants:
$$= \beta_1 \frac {\partial} {\partial x_1} x_{i1} + \beta_2 \frac {\partial} {\partial x_1} x_{i2} + \beta_3 \frac {\partial} {\partial x_1}x_{i2}^2 + \beta_4 \frac {\partial} {\partial x_1}x_{i2}^3 + \beta_5 \frac {\partial} {\partial x_1}x_{i3} + \beta_6 \frac {\partial} {\partial x_1}x_{i4} + \beta_7 \frac {\partial} {\partial x_1}x_{i3} \frac {\partial} {\partial x_1}x_{i4},$$
All other independent variables are treated as constants:
$$= \frac {\partial} {\partial x_1}\beta_1 x_{i1} + 1 (0) + 1(0) + 1(0) + 1(0) + 1(0) + 1(0)(0),$$
Bring the constant out:
$$= \beta_1 \frac {\partial} {\partial x_1} x_{i1} = \beta_1(1) = \beta_1.$$

## (b)

As long as the *x* variable is not exponentiated or multiplied, the coefficient is the partial derivative. This is not true for exponentiated terms. Take for example:
$$\frac {d}{dx} (a + \beta x^2),$$
$$= \frac {d}{dx} (a) + \frac {d}{dx}(\beta x^2),$$
$$= \beta \frac {d}{dx}(x^{2}),$$
$$= \beta (2 x^{2-1}),$$
$$=2\beta x.$$
This derivative does not yield the coefficient by itself, so you cannot interpret the coefficient as a straightforward linear relationship between *x* and *y* in the case of exponentiated terms. When the slope is exponential, the partial derivative gives you the instantaneous rate of change at each level of x which, unlike in the case of (a), will be different at each level of *x*.

Multiplicative relationships also cannot be interpreted as a straightfoward linear relationship. Consider a model with two predictors, $x_1$ and $x_2$, and their interaction: 
$$\frac {\partial}{\partial x_1} (a + \beta_1 x_1+ \beta_2 x_2 + \beta_{12}(x_1 \times x_2) = \beta1 + \beta_{12}x_2.$$
Here, the first derivative with respect to $x_1$ is a function of $x_{2}$. If there is an interaction, this partial derivative will change when evaluated across differing $x_2$ values. The key thing to notice is that because this function incorporates information about $x_2$, it does not describe an exclusive relationship between $x_1$ and $y$ like function (a) does.  Therefore, when an interaction term is present, the first partial derivaitve does not yield the regression coefficient $\beta_1$. However, if this derivative does not change across differing levels of $x_2$, there is no interaction. In cases where there is no interaction (i.e. in cases where $\beta_{12} x_2$ comes out to zero), this function *would*  describe the linear relationship between $x_1$ and $y$ in the way function (a) does. This would also be true whenever $x_2 = 0$. However, mathematically, this derivative does not directly represent the coefficient one sees in regression output for $x_1$ in a main-effects only model, nor does it yield the coefficient for the interaction. 

To get the coefficient for the interaction, you need to take the *cross-partial* derivative with respect to both $x_1$ and $x_2$:
$$\frac {\partial^2}{\partial x_1 x_2} (a + \beta_1 x_1+ \beta_2 x_2 + \beta_{12}(x_1 \times x_2) = \beta_{12}.$$
Importantly, this coefficient does not describe an exclusive relationship between $x_1$ and $y$ since it is a function of both $x_1$ and $x_2$. The cross-partial derivative introduces a third slope, which is the slope between $x_1$ and $y$ spanning across all levels of $x_2$. If this slope is zero, you have no interaction. 

In sum, the coefficient seen in regression output describes a linear relationship between $x$ and $y$ in a linear main-effects model but the meaning of this coefficient changes when $x$ is exponentiated or multiplied by another independent variable.  

## (c)

$$\frac {\partial} {\partial x_2} = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \beta_4 x_{i2}^3 + \beta_5 x_{i3} + \beta_6 x_{i4} + \beta_7 x_{i3} x_{i4},$$
Derivative of a constant is zero, so drop $\alpha$:
$$= \frac {\partial} {\partial x_2} (\beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \beta_4 x_{i2}^3 + \beta_5 x_{i3} + \beta_6 x_{i4} + \beta_7 x_{i3} x_{i4}),$$
Break up across addition and subtration, then again across multiplication:
$$= \frac {\partial} {\partial x_2}\beta_1  x_{i1} + \frac {\partial} {\partial x_2}\beta_2  x_{i2} + \frac {\partial} {\partial x_2}\beta_3 x_{i2}^2 + \frac {\partial} {\partial x_2}\beta_4 x_{i2}^3 + \frac {\partial} {\partial x_2}\beta_5 x_{i3} + \frac {\partial} {\partial x_2}\beta_6 x_{i4} + \frac {\partial} {\partial x_2}\beta_7 x_{i3} x_{i4},$$
The independent variables all get treated as constants:
$$= 0 + \frac {\partial} {\partial x_2}\beta_2  x_{i2} + \frac {\partial} {\partial x_2}\beta_3  x_{i2}^2 + \frac {\partial} {\partial x_2}\beta_4  x_{i2}^3 + 0 + 0 + 0 +0,$$
$$= \beta_2\frac {\partial} {\partial x_2} x_{i2} + \beta_3 \frac {\partial} {\partial x_2}  x_{i2}^2 + \beta_4 \frac {\partial} {\partial x_2}  x_{i2}^3,$$
$$= \beta_2 + 2\beta_3  x_{i2} + 3 \beta_4 x_{i2}^2.$$

## (d)

On the x-axis, you will age. On the y-axis, you will have: \newline
$$0.653 + 2(−0.073)x_{i2} + 3(0.00054)x_{i2}^2,$$
$$= 0.653 − 0.146x_{i2} + 0.00162x_{i2}^2.$$
Voting age starts at 18, so make age run from 18 to 100, let's say.

```{r, echo=FALSE}
x <- seq(18, 100, length=10000)
pd <- 0.653 - 0.146*x + 0.00162*x^2
df <- as.data.frame(cbind(x,pd))
ggplot2::ggplot(df, aes(x)) +
  geom_line(aes(y = pd)) +
  ylab("Age's Effect on Approval of Obama") + xlab("Age") +
  labs(color = "Function") +
  geom_hline(yintercept=0, linetype="dashed", color = "purple") +
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black"))
```

There is a curvilinear relationship such that generally speaking, age is negatively associated with approval. That is, the slope between age and approval is negative everywhere except for those older than around 80 years old or so. At that age, the effect turns around, and age becomes positively predictive of approval. The effect of age on approval is most negative at middle age.

## (e)

$$\frac {\partial} {\partial x_4} = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \beta_4 x_{i2}^3 + \beta_5 x_{i3} + \beta_6 x_{i4} + \beta_7 x_{i3} x_{i4},$$
Derivative of a constant is zero, so drop $\alpha$:
$$= \frac {\partial} {\partial x_4} (\beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \beta_4 x_{i2}^3 + \beta_5 x_{i3} + \beta_6 x_{i4} + \beta_7 x_{i3} x_{i4}),$$
$$= \frac {\partial} {\partial x_4} (\beta_1 x_{i1} + \frac {\partial} {\partial x_4}\beta_2 x_{i2} + \frac {\partial} {\partial x_4}\beta_3 x_{i2}^2 + \frac {\partial} {\partial x_4} \beta_4 x_{i2}^3 + \frac {\partial} {\partial x_4} \beta_5 x_{i3} + \frac {\partial} {\partial x_4}\beta_6 x_{i4} + \frac {\partial} {\partial x_4} \beta_7 x_{i3} x_{i4}),$$
$$= 0 + 0 + 0 + 0 + 0 + \frac {\partial} {\partial x_4} \beta_6 x_{i4} + \frac {\partial} {\partial x_4} \beta_7 x_{i3} x_{i4},$$
$$= \beta_6 \frac {\partial} {\partial x_4} x_{i4} + \beta_7 x_{i3} \frac {\partial} {\partial x_4} x_{i4},$$
$$= \beta_6 + \beta_7 x_{i3}.$$

## (f)

$$= \beta_6 + \beta_7 x_{i3} = 1 + 3 x_{i3}.$$
Dummy code sex and make male = 0 and female = 1:
$$\textrm{male} = 1 + 3(0) = 1.$$
$$\textrm{female} = 1 + 3(1) = 3.$$

## (g)

In both males and females, the derivative is positive in sign, indicating that there is a positive slope between pro-choice views and approval of Obama. However, there is also a sex difference: for females, the effect is 3 whereas for males it is 1; what this means is that the slope between abortion views and approval of Obama is steeper for women than for men. Because there is a difference in slopes of sex, we can say there is an interaction. In other words, there is a compounding effect of sex such that when you move from the male to female level, you see an increase in the strength of the relationship between abortion views and approval of Obama (descriptively speaking anyway).

# 9.10
```{r, echo=FALSE, results="asis"}
print.matrix <- function(m){
write.table(format(m, justify="right"),
            row.names=F, col.names=F, quote=F)
}
# This function lets you call R matrices while in latex math mode
matex <- function(x) {
  begin <- "\\begin{bmatrix}"
  end <- "\\end{bmatrix}"
  X <-
    apply(x, 1, function(x) {
      paste(
        paste(x, collapse = "&"),
        "\\\\"
      )
    })
  paste(c(begin, X, end), collapse = "")
}

y <- matrix(c(10, 7, 2, 8, 10, 6, 2, 10, 8, 5),
            nrow=10, ncol=1)
x <- matrix(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
              2, 1, 2, 0, 1, 3, 11, 0, 5, 0,
              5, 1, 0, 2, 0, 0, 0, 0, 12, 2),
            ncol = 3, nrow = 10)
x_prime <- t(x)
B <- matrix(c("\\alpha", "\\beta_1", "\\beta_2"), nrow=3, ncol=1)
E <- matrix(c("\\varepsilon_{\\textrm{USA}}", "\\varepsilon_{\\textrm{Peru}}", "\\varepsilon_{\\textrm{Brazil}}", "\\varepsilon_{\\textrm{Spain}}", "\\varepsilon_{\\textrm{Norway}}", "\\varepsilon_{\\textrm{Zimbabwe}}", "\\varepsilon_{\\textrm{Turkey}}", "\\varepsilon_{\\textrm{Japan}}", "\\varepsilon_{\\textrm{India}}", "\\varepsilon_{\\textrm{Sri \ Lanka}}"), nrow=10, ncol=1)
```

## (a)

$$
X'Y = `r matex(x_prime)` `r matex(y)`,
$$
$$
= 1(10) + 1(7) + 1(2) + 1(8) + 1(10) + 1(6) + 1(2) + 1(10) + 1(8) + 1(5) = 68,
$$
$$
\textrm{etc...}
$$
$$
= `r matex(x_prime %*% y)`.
$$

## (b)

$$
X'X = `r matex(x_prime)` \times `r matex(x)`,
$$
$$
1(1) + 1(1) + 1(1) + 1(1) + 1(1) + 1(1) + 1(1) + 1(1) + 1(1) + 1(1) = 10,
$$
$$
\textrm{etc...}
$$
$$
`r matex(x_prime %*% x)`.
$$

```{r, echo=FALSE}
# check work
x_prime_times_x <- x_prime %*% x
```


## (c)

```{r, echo=FALSE}
# modify the function to print determinants
print.matrix <- function(m){
write.table(format(m, justify="right"),
            row.names=F, col.names=F, quote=F)
}
# This function lets you call R matrices while in latex math mode
matex_det <- function(x) {
  begin <- "\\begin{vmatrix}"
  end <- "\\end{vmatrix}"
  X <-
    apply(x, 1, function(x) {
      paste(
        paste(x, collapse = "&"),
        "\\\\"
      )
    })
  paste(c(begin, X, end), collapse = "")
}

# 2 x 2's for computing minor elements
e_11 <- x_prime_times_x[-1,-1] # rm r1 & c1
e_12 <- x_prime_times_x[-1,-2] # rm r1 & c2
e_13 <- x_prime_times_x[-3,-1] # rm r3 & c1
e_21 <- x_prime_times_x[-2,-1] # rm r1 & c2
e_22 <- x_prime_times_x[-2,-2] # rm r2 & c2
e_23 <- x_prime_times_x[-2,-3] # rm r2 & c3
e_31 <- x_prime_times_x[-3,-1] # rm r3 & c1
e_32 <- x_prime_times_x[-2,-3] # rm r2 & c3
e_33 <- x_prime_times_x[-3,-3] # rm r3 & c3
```

```{r, echo=FALSE}
# check work
inverse_x_xprime <- round(solve(x_prime %*% x),3)
```

Follow the formula for the inverse of a large square matrix:
$$
\dfrac {1} {|A|} \textrm{adj}(A) = \dfrac {1} {|X'X|} \textrm{adj}(X'X).
$$
First we have to get the adjoint matrix. To get the adjoint matrix, we have to find the minor elements, followed by the cofactor matrix, then we can derive the adjoint matrix. 

To get the minor elements, we find the determinant of each possible 2 x 2 matrix. The determinant matrix is:
$$
\begin{vmatrix} a & b \\ c & d \end{vmatrix} = ad - bc,
$$
which we will have to do for all of the possible matrices. Starting with element (1,1):
$$
e_{11} = `r matex(e_11)` = 165(178) - 71(71) = 24,329.
$$
We continue this all the way down the line:
$$e_{12} = `r matex_det(e_12)` = `r det(e_12)`.$$
$$e_{13} = `r matex_det(e_13)` = `r det(e_13)`.$$
$$e_{21} = `r matex_det(e_21)` = `r det(e_21)`.$$
$$e_{22} = `r matex_det(e_22)` = `r det(e_22)`.$$
$$e_{23} = `r matex_det(e_23)` = `r det(e_23)`.$$
$$e_{31} = `r matex_det(e_31)` = `r det(e_31)`.$$
$$e_{32} = `r matex_det(e_32)` = `r det(e_32)`.$$
$$e_{33} = `r matex_det(e_33)` = `r det(e_33)`.$$
```{r, echo=FALSE}
# make matrix of minor elements
MEs <- matrix(c(det(e_11), det(e_12), det(e_13),
                det(e_21), det(e_22), det(e_23),
                det(e_31), det(e_32), det(e_33)),
              nrow = 3, ncol = 3)

# make cofactor matrix
cofactor <- matrix(c(det(e_11), (-1 * det(e_12)), det(e_13),
                (-1 * det(e_21)), det(e_22), (-1 * det(e_23)),
                det(e_31), (-1 * det(e_32)), det(e_33)),
              nrow = 3, ncol = 3)
```
$$\textrm{Minor Elements Matrix} = `r matex(MEs)`.$$

To get the cofactor matrix, you just multiple the cells by -1 if their row and columns number add up to an odd number:
$$\textrm{Cofactor Matrix} = `r matex(cofactor)`.$$

Lastly, you derive the adjoint matrix by transposing the cofactor matrix. But since it's symmetric, the cofactor matrix equals the adjoint matrix:
$$\textrm{Adjoint Matrix} = `r matex(t(cofactor))`.$$

Now that we have the adjoint matrix, we use it to find the determinant of the $X'X$ matrix. We do this by multiplying a row or column of $X'X$ by its corresponding elements in the cofactor matrix and then sum the product. 

```{r, echo=FALSE}
# check work
xpx_col_1 <- matrix(x_prime_times_x[,2])
cof_col1 <- matrix(cofactor[,2])
prod <- xpx_col_1 * cof_col1
sum <- sum(prod)
```

$$`r matex(xpx_col_1)` \times `r matex(cof_col1)` =  \sum `r matex(prod)` = `r sum`.$$

Last, now that we have $|X'X|$, we can use the formula for matrix inversion:
```{r, echo=FALSE}
inverse <- (1/sum) * cofactor
```

$$
(X'X)^{-1} = \dfrac {1}{|X'X|}\textrm{adj}(X'X),
$$
$$
= \dfrac{1}{`r sum`} \times `r matex(cofactor)` = `r matex(round(inverse,3))`.
$$

```{r, echo=FALSE}
# check work with the inverse function
solve <- solve(x_prime_times_x)
```

## (d)

$$
\hat \beta = (X'X)^{-1}(X'Y),
$$
$$
=`r matex(round(inverse,3))` \times `r matex(x_prime %*% y)`,
$$
$$
= `r matex(round(inverse %*% (x_prime %*% y),3))` = `r matex(B)`. 
$$

## Check with Regression

```{r, echo=FALSE, results="asis"}
demo_data <- data.frame(c(c("USA", "Peru", "Brazil", "Spain", 
                          "Norway", "Zimbabwe", "Turkey", 
                          "Japan", "India", "Sri Lanka")),
                          y, x[,2], x[,3])
colnames(demo_data) <- c("Country", "Democracy", "Sanctions", "Riots")

fit <- lm(Democracy ~ Sanctions + Riots, 
          data = demo_data)
xtable::xtable(fit, align = "lrrrr")
```





# 10.9

## (a)

#### Characterization:
Social capital describes the level of cohesion members of a community have. It can be characterized as falling along two dimensions: trust in other members of the community and willingness to cooperate with others. That is, social capital is a function of both ambient levels of trust in a community as well as the general tendency to cooperate civically, such as by returning found money or being willing to contribute to public welfare through taxes and fares. Communities with the most social capital will on average score highest on both dimensions while those with lowest social capital will score lowest.

#### Representation:
To represent this construct, we'd need to identify indicators of the construct. What kinds of thoughts, experiences, and behaviors do individuals of a community with high social capital tend to do? The items should reflect these. Trust is represented by a single-item measure where the top of the scale represents the attitude that most people can be trusted and the low end represents the attitude that most people cannot be trusted. Those high in the trust dimension of social capital should score toward the high end of the scale and those low in trust should score toward the low end of the scale. Civic cooperation is represented by a set of statements describing situations where one could act for the benefit of the community but chose not to. The high end of the scale represents the attitude that these behaviors are never acceptable while the low represents the attitude that they are always acceptable. Those high in civic cooperation should score toward the high end of the scale and those low should score on the low end. 

#### Measurement:
The measurement model is a principal components analysis in this case. 

## (b)

```{r, echo=FALSE}
covar <- matrix(c(6.32, 4.47, 4.47, 7.51), 
                ncol = 2, nrow = 2)
```

The covariance matrix of the data is:
$$`r matex(covar)`$$

Find the eigenvalues and unit eigenvectors of the covariance matrix.


There are 4 steps for finding the eigenvalues of a square matrix. The first step is to write out the matrix $(A - \lambda I)$, then find the determinant of this matrix $|A - \lambda I|$ in terms of $\lambda$:
$$`r matex(covar)` \times \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \lambda \begin{bmatrix} x_1 \\ x_2 \end{bmatrix},$$
$$\Bigg( `r matex(covar)` - \lambda  \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \Bigg) \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix},$$
$$\Bigg( `r matex(covar)` -  \begin{bmatrix} \lambda & 0 \\ 0 & \lambda \end{bmatrix} \Bigg) \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix},$$
$$\Bigg( \begin{bmatrix}  6.32 - \lambda & 4.47 \\ 4.47 & 7.51 -\lambda \end{bmatrix} \Bigg) \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix},$$
$$\begin{bmatrix}  6.32 - \lambda & 4.47 \\ 4.47 & 7.51 -\lambda \end{bmatrix}.$$
Set the determinant equal to 0, then solve for $\lambda$:
$$\begin{vmatrix}  6.32 - \lambda & 4.47 \\ 4.47 & 7.51 -\lambda \end{vmatrix} = 0,$$
$$(6.32 − \lambda)(7.51 − \lambda) − 4.47^2 = 0,$$
$$(6.32 − \lambda)(7.51 − \lambda) − 4.47^2 = 0,$$
Let python do the algebra - eigenvalues equal:
```{python, echo=FALSE}
# Check Work
import sympy
from sympy import *
from sympy import Symbol
x = Symbol('x')
#Solve
lamb = solveset(Eq((6.32 - x)*(7.51 - x) - 4.47**2, 0),x)
for a in preorder_traversal(lamb):
    if isinstance(a, Float):
        lamb = lamb.subs(a, round(a, 2))
print("solveset(Eq((6.32 - x)*(7.51 - x) - 4.47**2, 0),x)")
print(lamb)
```

To get the eigenvectors, start by substituting these back in for $\lambda$ and reduce them:
$$\begin{bmatrix}  6.32 - \lambda & 4.47 \\ 4.47 & 7.51 -\lambda \end{bmatrix},$$
$$= \begin{bmatrix}  6.32 - 2.41 & 4.47 \\ 4.47 & 7.51 - 2.41 \end{bmatrix},$$
$$= \begin{bmatrix}  3.91 & 4.47 \\ 4.47 & 5.10 \end{bmatrix}.$$
Multiply the second row by -0.87 ($\frac {-4.47}{5.10} = -0.87$):
$$= \begin{bmatrix}  0 & 0 \\ 4.47 & 5.10 \end{bmatrix}.$$
Because we made $(A - \lambda I) = 0$ earlier, we know the system of equations will be:
$$\begin{bmatrix}  0 & 0 \\ 4.47 & 5.10 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix},$$
$$= \begin{cases} x = x, \\ 4.47x + 5.10y = 0, \end{cases}$$
$$= \begin{cases} x = x, \\ 5.10y = -4.47x, \end{cases}$$
$$= \begin{cases} x = x, \\ y = -0.87x, \end{cases}$$
$$= \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} x \\ -0.87x \end{bmatrix}=  x\begin{bmatrix} 1 \\ -0.87 \end{bmatrix}.$$
To get unit eigenvector, get the vector length and divide the vector by its length:
$$\begin{vmatrix} 1 \\ -0.87 \end{vmatrix} = \sqrt{1^2 + (-0.87)^2} = `r round(sqrt((1^2 + (-0.87)^2)),2)`.$$
```{r, echo=FALSE}
ev1 <-( 1/1.33) * matrix(c(1, -0.87), nrow=2, ncol=1)
ev1 <- round(ev1, 2)
```
$$\dfrac {1} {1.33} \begin{bmatrix} 1 \\ -0.87 \end{bmatrix} = `r matex(ev1)`.$$



Now for the other one:
$$= \begin{bmatrix}  6.32 - 11.42 & 4.47 \\ 4.47 & 7.51 - 11.42 \end{bmatrix},$$
$$= \begin{bmatrix}  - 5.10 & 4.47 \\ 4.47 & - 3.91 \end{bmatrix},$$
$$= \begin{bmatrix}  0 & 0 \\ 4.47 & -3.91 \end{bmatrix},$$
$$= \begin{cases} x = x, \\ 4.47x - 3.91y = 0, \end{cases}$$
$$= \begin{cases} x = x, \\ - 3.91y = -4.47x, \end{cases}$$
$$= \begin{cases} x = x, \\ y = 1.14x, \end{cases}$$
$$= \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} x \\ 1.14x \end{bmatrix}=  x\begin{bmatrix} 1 \\ 1.14 \end{bmatrix}.$$
To get unit eigenvector, get the vector length and divide the vector by its length:
$$\begin{vmatrix} 1 \\ 1.14 \end{vmatrix} = \sqrt{1^2 + 1.14^2} = `r round(sqrt((1^2 + 1.14^2)),2)`.$$
```{r, echo=FALSE}
ev2 <-( 1/1.56) * matrix(c(1, 1.14), nrow=2, ncol=1)
ev2 <- round(ev2, 2)
```
$$\dfrac {1} {1.56} \begin{bmatrix} 1 \\ 1.14 \end{bmatrix} = `r matex(ev2)`.$$

## (c)

