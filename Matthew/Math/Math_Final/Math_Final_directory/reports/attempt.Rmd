---
title             : "The title"
shorttitle        : "Title"

author: 
  - name          : "First Author"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
  - name          : "Ernst-August Doelle"
    affiliation   : "1,2"

affiliation:
  - id            : "1"
    institution   : "Wilhelm-Wundt-University"
  - id            : "2"
    institution   : "Konstanz Business School"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

`r knitr::opts_knit$set(root.dir='..')`

```{r, include=FALSE}
library(ProjectTemplate); load.project()
library(papaja)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
options(scipen = 999)
knitr::opts_chunk$set(engine.path = "/anaconda3/bin/python")
```


# 7.11

## (a)

$$\frac {\partial} {\partial x_1} = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \beta_4 x_{i2}^3 + \beta_5 x_{i3} + \beta_6 x_{i4} + \beta_7 x_{i3} x_{i4},$$
Derivative of a constant is zero, so drop $\alpha$:
$$= \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \beta_4 x_{i2}^3 + \beta_5 x_{i3} + \beta_6 x_{i4} + \beta_7 x_{i3} x_{i4},$$
Break up across addition and subtraction, then bring out constants:
$$= \beta_1 \frac {\partial} {\partial x_1} x_{i1} + \beta_2 \frac {\partial} {\partial x_1} x_{i2} + \beta_3 \frac {\partial} {\partial x_1}x_{i2}^2 + \beta_4 \frac {\partial} {\partial x_1}x_{i2}^3 + \beta_5 \frac {\partial} {\partial x_1}x_{i3} + \beta_6 \frac {\partial} {\partial x_1}x_{i4} + \beta_7 \frac {\partial} {\partial x_1}x_{i3} \frac {\partial} {\partial x_1}x_{i4},$$
All other independent variables are treated as constants:
$$= \frac {\partial} {\partial x_1}\beta_1 x_{i1} + 1 (0) + 1(0) + 1(0) + 1(0) + 1(0) + 1(0)(0),$$
Bring the constant out:
$$= \beta_1 \frac {\partial} {\partial x_1} x_{i1} = \beta_1(1) = \beta_1.$$

## (b)

As long as the *x* variable is not exponentiated or multiplied, the coefficient is the partial derivative. This is not true for exponentiated terms. Take for example:
$$\frac {d}{dx} (a + \beta x^2),$$
$$= \frac {d}{dx} (a) + \frac {d}{dx}(\beta x^2),$$
$$= \beta \frac {d}{dx}(x^{2}),$$
$$= \beta (2 x^{2-1}),$$
$$=2\beta x.$$
This derivative does not yield the coefficient by itself, so you cannot interpret the coefficient as a straightforward linear relationship between *x* and *y* in the case of exponentiated terms. When the slope is exponential, the partial derivative gives you the instantaneous rate of change at each level of x which, unlike in the case of (a), will be different at each level of *x*.

Multiplicative relationships also cannot be interpreted as a straightfoward linear relationship. Consider a model with two predictors, $x_1$ and $x_2$, and their interaction: 
$$\frac {\partial}{\partial x_1} (a + \beta_1 x_1+ \beta_2 x_2 + \beta_{12}(x_1 \times x_2) = \beta1 + \beta_{12}x_2.$$
Here, the first derivative with respect to $x_1$ is a function of $x_{2}$. If there is an interaction, this partial derivative will change when evaluated across differing $x_2$ values. The key thing to notice is that because this function incorporates information about $x_2$, it does not describe an exclusive relationship between $x_1$ and $y$ like function (a) does.  Therefore, when an interaction term is present, the first partial derivaitve does not yield the regression coefficient $\beta_1$. However, if this derivative does not change across differing levels of $x_2$, there is no interaction. In cases where there is no interaction (i.e. in cases where $\beta_{12} x_2$ comes out to zero), this function *would*  describe the linear relationship between $x_1$ and $y$ in the way function (a) does. This would also be true whenever $x_2 = 0$. However, mathematically, this derivative does not directly represent the coefficient one sees in regression output for $x_1$ in a main-effects only model, nor does it yield the coefficient for the interaction. 

To get the coefficient for the interaction, you need to take the *cross-partial* derivative with respect to both $x_1$ and $x_2$:
$$\frac {\partial^2}{\partial x_1 x_2} (a + \beta_1 x_1+ \beta_2 x_2 + \beta_{12}(x_1 \times x_2) = \beta_{12}.$$
Importantly, this coefficient does not describe an exclusive relationship between $x_1$ and $y$ since it is a function of both $x_1$ and $x_2$. The cross-partial derivative introduces a third slope, which is the slope between $x_1$ and $y$ spanning across all levels of $x_2$. If this slope is zero, you have no interaction. 

In sum, the coefficient seen in regression output describes a linear relationship between $x$ and $y$ in a linear main-effects model but the meaning of this coefficient changes when $x$ is exponentiated or multiplied by another independent variable.  

## (c)

$$\frac {\partial} {\partial x_2} = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \beta_4 x_{i2}^3 + \beta_5 x_{i3} + \beta_6 x_{i4} + \beta_7 x_{i3} x_{i4},$$
Derivative of a constant is zero, so drop $\alpha$:
$$= \frac {\partial} {\partial x_2} (\beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \beta_4 x_{i2}^3 + \beta_5 x_{i3} + \beta_6 x_{i4} + \beta_7 x_{i3} x_{i4}),$$
Break up across addition and subtration, then again across multiplication:
$$= \frac {\partial} {\partial x_2}\beta_1  x_{i1} + \frac {\partial} {\partial x_2}\beta_2  x_{i2} + \frac {\partial} {\partial x_2}\beta_3 x_{i2}^2 + \frac {\partial} {\partial x_2}\beta_4 x_{i2}^3 + \frac {\partial} {\partial x_2}\beta_5 x_{i3} + \frac {\partial} {\partial x_2}\beta_6 x_{i4} + \frac {\partial} {\partial x_2}\beta_7 x_{i3} x_{i4},$$
The independent variables all get treated as constants:
$$= 0 + \frac {\partial} {\partial x_2}\beta_2  x_{i2} + \frac {\partial} {\partial x_2}\beta_3  x_{i2}^2 + \frac {\partial} {\partial x_2}\beta_4  x_{i2}^3 + 0 + 0 + 0 +0,$$
$$= \beta_2\frac {\partial} {\partial x_2} x_{i2} + \beta_3 \frac {\partial} {\partial x_2}  x_{i2}^2 + \beta_4 \frac {\partial} {\partial x_2}  x_{i2}^3,$$
$$= \beta_2 + 2\beta_3  x_{i2} + 3 \beta_4 x_{i2}^2.$$

## (d)

On the x-axis, you will age. On the y-axis, you will have: \newline
$$0.653 + 2(−0.073)x_{i2} + 3(0.00054)x_{i2}^2,$$
$$= 0.653 − 0.146x_{i2} + 0.00162x_{i2}^2.$$
Voting age starts at 18, so make age run from 18 to 100, let's say.

```{r, echo=FALSE}
x <- seq(18, 100, length=10000)
pd <- 0.653 - 0.146*x + 0.00162*x^2
df <- as.data.frame(cbind(x,pd))
ggplot(df, aes(x)) +
  geom_line(aes(y = pd)) +
  ylab("Age's Effect on Approval of Obama") + xlab("Age") +
  labs(color = "Function") +
  geom_hline(yintercept=0, linetype="dashed", color = "purple") +
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black"))
```

There is a curvilinear relationship such that generally speaking, age is negatively associated with approval. That is, the slope between age and approval is negative everywhere except for those older than around 80 years old or so. At that age, the effect turns around, and age becomes positively predictive of approval. The effect of age on approval is most negative at middle age.

## (e)

$$\frac {\partial} {\partial x_4} = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \beta_4 x_{i2}^3 + \beta_5 x_{i3} + \beta_6 x_{i4} + \beta_7 x_{i3} x_{i4},$$
Derivative of a constant is zero, so drop $\alpha$:
$$= \frac {\partial} {\partial x_4} (\beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \beta_4 x_{i2}^3 + \beta_5 x_{i3} + \beta_6 x_{i4} + \beta_7 x_{i3} x_{i4}),$$
$$= \frac {\partial} {\partial x_4} (\beta_1 x_{i1} + \frac {\partial} {\partial x_4}\beta_2 x_{i2} + \frac {\partial} {\partial x_4}\beta_3 x_{i2}^2 + \frac {\partial} {\partial x_4} \beta_4 x_{i2}^3 + \frac {\partial} {\partial x_4} \beta_5 x_{i3} + \frac {\partial} {\partial x_4}\beta_6 x_{i4} + \frac {\partial} {\partial x_4} \beta_7 x_{i3} x_{i4}),$$
$$= 0 + 0 + 0 + 0 + 0 + \frac {\partial} {\partial x_4} \beta_6 x_{i4} + \frac {\partial} {\partial x_4} \beta_7 x_{i3} x_{i4},$$
$$= \beta_6 \frac {\partial} {\partial x_4} x_{i4} + \beta_7 x_{i3} \frac {\partial} {\partial x_4} x_{i4},$$
$$= \beta_6 + \beta_7 x_{i3}.$$

## (f)

$$= \beta_6 + \beta_7 x_{i3} = 1 + 3 x_{i3}.$$
Dummy code sex and make male = 0 and female = 1:
$$\textrm{male} = 1 + 3(0) = 1.$$
$$\textrm{female} = 1 + 3(1) = 3.$$

## (g)

In both males and females, the derivative is positive in sign, indicating that there is a positive slope between pro-choice views and approval of Obama. However, there is also a sex difference: for females, the effect is 3 whereas for males it is 1; what this means is that the slope between abortion views and approval of Obama is steeper for women than for men. Because there is a difference in slopes of sex, we can say there is an interaction. In other words, there is a compounding effect of sex such that when you move from the male to female level, you see an increase in the strength of the relationship between abortion views and approval of Obama (descriptively speaking anyway).

# 9.10
```{r, echo=FALSE, results="asis"}
print.matrix <- function(m){
write.table(format(m, justify="right"),
            row.names=F, col.names=F, quote=F)
}
# This function lets you call R matrices while in latex math mode
matex <- function(x) {
  begin <- "\\begin{bmatrix}"
  end <- "\\end{bmatrix}"
  X <-
    apply(x, 1, function(x) {
      paste(
        paste(x, collapse = "&"),
        "\\\\"
      )
    })
  paste(c(begin, X, end), collapse = "")
}

y <- matrix(c(10, 7, 2, 8, 10, 6, 2, 10, 8, 5),
            nrow=10, ncol=1)
x <- matrix(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
              2, 1, 2, 0, 1, 3, 11, 0, 5, 0,
              5, 1, 0, 2, 0, 0, 0, 0, 12, 2),
            ncol = 3, nrow = 10)
x_prime <- t(x)
B <- matrix(c("\\alpha", "\\beta_1", "\\beta_2"), nrow=3, ncol=1)
E <- matrix(c("\\varepsilon_{\\textrm{USA}}", "\\varepsilon_{\\textrm{Peru}}", "\\varepsilon_{\\textrm{Brazil}}", "\\varepsilon_{\\textrm{Spain}}", "\\varepsilon_{\\textrm{Norway}}", "\\varepsilon_{\\textrm{Zimbabwe}}", "\\varepsilon_{\\textrm{Turkey}}", "\\varepsilon_{\\textrm{Japan}}", "\\varepsilon_{\\textrm{India}}", "\\varepsilon_{\\textrm{Sri \ Lanka}}"), nrow=10, ncol=1)
```

## (a)

$$
X'Y = `r matex(x_prime)` `r matex(y)`,
$$
$$
= 1(10) + 1(7) + 1(2) + 1(8) + 1(10) + 1(6) + 1(2) + 1(10) + 1(8) + 1(5) = 68,
$$
$$
\textrm{etc...}
$$
$$
= `r matex(x_prime %*% y)`.
$$

## (b)

$$
X'X = `r matex(x_prime)` \times `r matex(x)`,
$$
$$
1(1) + 1(1) + 1(1) + 1(1) + 1(1) + 1(1) + 1(1) + 1(1) + 1(1) + 1(1) = 10,
$$
$$
\textrm{etc...}
$$
$$
`r matex(x_prime %*% x)`.
$$

```{r, echo=FALSE}
# check work
x_prime_times_x <- x_prime %*% x
```


## (c)

```{r, echo=FALSE}
# modify the function to print determinants
print.matrix <- function(m){
write.table(format(m, justify="right"),
            row.names=F, col.names=F, quote=F)
}
# This function lets you call R matrices while in latex math mode
matex_det <- function(x) {
  begin <- "\\begin{vmatrix}"
  end <- "\\end{vmatrix}"
  X <-
    apply(x, 1, function(x) {
      paste(
        paste(x, collapse = "&"),
        "\\\\"
      )
    })
  paste(c(begin, X, end), collapse = "")
}

# 2 x 2's for computing minor elements
e_11 <- x_prime_times_x[-1,-1] # rm r1 & c1
e_12 <- x_prime_times_x[-1,-2] # rm r1 & c2
e_13 <- x_prime_times_x[-3,-1] # rm r3 & c1
e_21 <- x_prime_times_x[-2,-1] # rm r1 & c2
e_22 <- x_prime_times_x[-2,-2] # rm r2 & c2
e_23 <- x_prime_times_x[-2,-3] # rm r2 & c3
e_31 <- x_prime_times_x[-3,-1] # rm r3 & c1
e_32 <- x_prime_times_x[-2,-3] # rm r2 & c3
e_33 <- x_prime_times_x[-3,-3] # rm r3 & c3
```

```{r, echo=FALSE}
# check work
inverse_x_xprime <- round(solve(x_prime %*% x),3)
```

Follow the formula for the inverse of a large square matrix:
$$
\dfrac {1} {|A|} \textrm{adj}(A) = \dfrac {1} {|X'X|} \textrm{adj}(X'X).
$$
First we have to get the adjoint matrix. To get the adjoint matrix, we have to find the minor elements, followed by the cofactor matrix, then we can derive the adjoint matrix. 

To get the minor elements, we find the determinant of each possible 2 x 2 matrix. The determinant matrix is:
$$
\begin{vmatrix} a & b \\ c & d \end{vmatrix} = ad - bc,
$$
which we will have to do for all of the possible matrices. Starting with element (1,1):
$$
e_{11} = `r matex(e_11)` = 165(178) - 71(71) = 24,329.
$$
We continue this all the way down the line:
$$e_{12} = `r matex_det(e_12)` = `r det(e_12)`.$$
$$e_{13} = `r matex_det(e_13)` = `r det(e_13)`.$$
$$e_{21} = `r matex_det(e_21)` = `r det(e_21)`.$$
$$e_{22} = `r matex_det(e_22)` = `r det(e_22)`.$$
$$e_{23} = `r matex_det(e_23)` = `r det(e_23)`.$$
$$e_{31} = `r matex_det(e_31)` = `r det(e_31)`.$$
$$e_{32} = `r matex_det(e_32)` = `r det(e_32)`.$$
$$e_{33} = `r matex_det(e_33)` = `r det(e_33)`.$$
```{r, echo=FALSE}
# make matrix of minor elements
MEs <- matrix(c(det(e_11), det(e_12), det(e_13),
                det(e_21), det(e_22), det(e_23),
                det(e_31), det(e_32), det(e_33)),
              nrow = 3, ncol = 3)

# make cofactor matrix
cofactor <- matrix(c(det(e_11), (-1 * det(e_12)), det(e_13),
                (-1 * det(e_21)), det(e_22), (-1 * det(e_23)),
                det(e_31), (-1 * det(e_32)), det(e_33)),
              nrow = 3, ncol = 3)
```
$$\textrm{Minor Elements Matrix} = `r matex(MEs)`.$$

To get the cofactor matrix, you just multiple the cells by -1 if their row and columns number add up to an odd number:
$$\textrm{Cofactor Matrix} = `r matex(cofactor)`.$$

Lastly, you derive the adjoint matrix by transposing the cofactor matrix. But since it's symmetric, the cofactor matrix equals the adjoint matrix:
$$\textrm{Adjoint Matrix} = `r matex(t(cofactor))`.$$

Now that we have the adjoint matrix, we use it to find the determinant of the $X'X$ matrix. We do this by multiplying a row or column of $X'X$ by its corresponding elements in the cofactor matrix and then sum the product. 

```{r, echo=FALSE}
# check work
xpx_col_1 <- matrix(x_prime_times_x[,2])
cof_col1 <- matrix(cofactor[,2])
prod <- xpx_col_1 * cof_col1
sum <- sum(prod)
```

$$`r matex(xpx_col_1)` \times `r matex(cof_col1)` =  \sum `r matex(prod)` = `r sum`.$$

Last, now that we have $|X'X|$, we can use the formula for matrix inversion:
```{r, echo=FALSE}
inverse <- (1/sum) * cofactor
```

$$
(X'X)^{-1} = \dfrac {1}{|X'X|}\textrm{adj}(X'X),
$$
$$
= \dfrac{1}{`r sum`} \times `r matex(cofactor)` = `r matex(round(inverse,3))`.
$$

```{r, echo=FALSE}
# check work with the inverse function
solve <- solve(x_prime_times_x)
```

## (d)

$$
\hat \beta = (X'X)^{-1}(X'Y),
$$
$$
=`r matex(round(inverse,3))` \times `r matex(x_prime %*% y)`,
$$
$$
= `r matex(round(inverse %*% (x_prime %*% y),3))` = `r matex(B)`. 
$$

## Check with Regression

```{r, echo=FALSE, results="asis"}
demo_data <- data.frame(c(c("USA", "Peru", "Brazil", "Spain", 
                          "Norway", "Zimbabwe", "Turkey", 
                          "Japan", "India", "Sri Lanka")),
                          y, x[,2], x[,3])
colnames(demo_data) <- c("Country", "Democracy", "Sanctions", "Riots")

fit <- lm(Democracy ~ Sanctions + Riots, 
          data = demo_data)
papaja::apa_print(fit)

```



# 10.9
